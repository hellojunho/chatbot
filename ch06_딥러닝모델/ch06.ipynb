{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6장. 딥러닝 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2100/2100 [==============================] - 31s 15ms/step - loss: 0.7929 - accuracy: 0.7742 - val_loss: 0.3502 - val_accuracy: 0.90041s -\n"
     ]
    }
   ],
   "source": [
    "# mnist 분류 모델 학습\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "\n",
    "# MNIST 데이터셋 가져오기\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 # 데이터 정규화\n",
    "\n",
    "# tf.data를 사용하여 데이터셋을 섞고 배치 만들기\n",
    "ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000)\n",
    "train_size = int(len(x_train) * 0.7) # 학슴셋 : 검증셋 = 7 : 3\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).batch(20)\n",
    "\n",
    "# MNIST 분류 모델 구성\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28)))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# 모델 생성\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "# model.complie(loss='categorial_crossentropy', optimizer='sgd', metrics['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "hist = model.fit(train_ds, validation_data=val_ds, epochs=10)\n",
    "\n",
    "# 모델 평가\n",
    "print('모델 평가')\n",
    "model.evaluate(x_test, y_test)\n",
    "\n",
    "# 모델 정보 출력\n",
    "model.summary()\n",
    "\n",
    "# 모델 저장\n",
    "model.save('mnist_model.h5')\n",
    "\n",
    "# 학습 결과 그래프 그리기\n",
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['var_loss'], 'r', label='var loss')\n",
    "\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.3 학습된 딥러닝 모델 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: mnist_model.h5/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-99142d80ae70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 모델 불러오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m       \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    111\u001b[0m                   (export_dir,\n\u001b[1;32m    112\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: mnist_model.h5/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "# mnist 분류 모델 파일 불러오기\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MNIST 데이터셋 가져오기\n",
    "_, (x_test, y_test) = mnist.load_data()\n",
    "x_test = x_test // 255.0 # 데이터 정규화\n",
    "\n",
    "# 모델 불러오기 - 이 전에 mnist_model.h5가 생성되어야 함...\n",
    "model = load_model('mnist_model.h5')\n",
    "model.summary()\n",
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "# 테스트셋에서 20번째 이미지 출력\n",
    "plt.imshow(x_test[20], cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# 테스트셋의 20번째 이미지 클래스 분류\n",
    "picks = [20]\n",
    "predict = model.predict_classes(x_test[picks])\n",
    "print(\"손글씨 이미지 예측값 : \", predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.1 CNN모델 개념"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 876ms/step - loss: 0.9991 - accuracy: 0.7143 - val_loss: 0.8876 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7420 - accuracy: 1.0000 - val_loss: 0.7063 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.5267 - accuracy: 1.0000 - val_loss: 0.5538 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.4077 - accuracy: 1.0000 - val_loss: 0.3893 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.2173 - accuracy: 1.0000 - val_loss: 0.2929 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2683 - accuracy: 1.0000\n",
      "Accuarcy: 100.000000\n",
      "loss: 0.268282\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-62e4c8d3a8bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# 모델 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cnn_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1977\u001b[0m     \"\"\"\n\u001b[1;32m   1978\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1979\u001b[0;31m                     signatures, options)\n\u001b[0m\u001b[1;32m   1980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1981\u001b[0m   def save_weights(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    129\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    130\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 131\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    132\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mmodel_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# TODO(b/128683857): Add integration tests between tf.keras and external\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0msave_attributes_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m       \u001b[0mparam_dset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                     \u001b[0mh5objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "# 문장 감정 분류 CNN 모델\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "\n",
    "# 데이터 읽어오기\n",
    "train_file = \"./chatbot_data.csv\"\n",
    "data = pd.read_csv(train_file, delimiter=',')\n",
    "features = data['Q'].tolist()\n",
    "labels = data['label'].tolist()\n",
    "\n",
    "# 단어 인덱스 시퀀스 벡터\n",
    "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "MAX_SEQ_LEN = 15\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "# 학습용, 검증용, 테스트용 데이터셋 생성\n",
    "# 학습셋:검증셋:테스트셋 = 7:2:1\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
    "ds = ds.shuffle(len(features))\n",
    "\n",
    "train_size = int(len(padded_seqs) * 0.7)\n",
    "val_size = int(len(padded_seqs) * 0.2)\n",
    "test_size = int(len(padded_seqs) * 0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.take(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.take(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(word_index) + 1 # 전체 단어 수\n",
    "\n",
    "# CNN 모델 정의\n",
    "input_layer = Input(shape=(MAX_SEQ_LEN, ))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "conv1 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=3,\n",
    "    padding='valid',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "\n",
    "conv2 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=4,\n",
    "    padding='valid',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "\n",
    "conv3 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=5,\n",
    "    padding='valid',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "# 3, 4, 5 5-gram 이후 병합\n",
    "concat = concatenate([pool1, pool2, pool3])\n",
    "\n",
    "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "logits = Dense(3, name='logits')(dropout_hidden)\n",
    "predictions = Dense(3, activation=tf.nn.softmax)(logits)\n",
    "\n",
    "# 모델 생성\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)\n",
    "\n",
    "# 모델 평가(테스트 데이터셋 이용)\n",
    "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "print('Accuarcy: %f' %(accuracy * 100))\n",
    "print('loss: %f' %(loss))\n",
    "\n",
    "# 모델 저장\n",
    "model.save('cnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.3 챗봇 문답 데이터 감정 분류 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 601ms/step - loss: 1.1847 - accuracy: 0.0000e+00 - val_loss: 0.9417 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.8784 - accuracy: 1.0000 - val_loss: 0.8195 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.7025 - accuracy: 1.0000 - val_loss: 0.7071 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.5296 - accuracy: 1.0000 - val_loss: 0.5860 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4407 - accuracy: 1.0000 - val_loss: 0.4804 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4694 - accuracy: 1.0000\n",
      "Accuracy: 100.000000\n",
      "loss: 0.469418\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b7b2084a8fef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# 모델 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cnn_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1977\u001b[0m     \"\"\"\n\u001b[1;32m   1978\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1979\u001b[0;31m                     signatures, options)\n\u001b[0m\u001b[1;32m   1980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1981\u001b[0m   def save_weights(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    129\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    130\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 131\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    132\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mmodel_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# TODO(b/128683857): Add integration tests between tf.keras and external\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0msave_attributes_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m       \u001b[0mparam_dset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                     \u001b[0mh5objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "\n",
    "# 데이터 읽어오기\n",
    "train_file = \"./chatbot_data.csv\"\n",
    "data = pd.read_csv(train_file, delimiter=',')\n",
    "features = data['Q'].tolist()\n",
    "labels = data['label'].tolist()\n",
    "\n",
    "# 단어 인덱스 시퀀스 벡터\n",
    "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "word_index = tokenizer.word_index\n",
    "MAX_SEQ_LEN = 15  # 단어 시퀀스 벡터 크기\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "# 학습용, 검증용, 테스트용 데이터셋 생성 ➌\n",
    "# 학습셋:검증셋:테스트셋 = 7:2:1\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
    "ds = ds.shuffle(len(features))\n",
    "train_size = int(len(padded_seqs) * 0.7)\n",
    "val_size = int(len(padded_seqs) * 0.2)\n",
    "test_size = int(len(padded_seqs) * 0.1)\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(word_index) + 1  # 전체 단어 수\n",
    "\n",
    "# CNN 모델 정의\n",
    "input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "conv1 = Conv1D(filters=128, kernel_size=3, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "conv2 = Conv1D(filters=128, kernel_size=4, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "conv3 = Conv1D(filters=128, kernel_size=5, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "# 3, 4, 5- gram 이후 합치기\n",
    "concat = concatenate([pool1, pool2, pool3])\n",
    "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "logits = Dense(3, name='logits')(dropout_hidden)\n",
    "predictions = Dense(3, activation=tf.nn.softmax)(logits)\n",
    "\n",
    "# 모델 생성\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)\n",
    "\n",
    "# 모델 평가(테스트 데이터셋 이용)\n",
    "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy * 100))\n",
    "print('loss: %f' % (loss))\n",
    "\n",
    "# 모델 저장\n",
    "model.save('cnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 6-5 곡선 에측  RNN 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape x:(185, 15) / y:(185,)\n",
      "train_x.shape = (185, 15, 1)\n",
      "train_y.shape = (185,)\n",
      "Epoch 1/1000\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 1.9728\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.6126\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.3126\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.0672\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.8535\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.6842\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.5347\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.4156\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.3184\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.2380\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.1778\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1337\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1035\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0834\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0707\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0627\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0577\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0541\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0512\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0484\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0458\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0434\n",
      "Epoch 23/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0410\n",
      "Epoch 24/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0391\n",
      "Epoch 25/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0371\n",
      "Epoch 26/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0356\n",
      "Epoch 27/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0341\n",
      "Epoch 28/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0326\n",
      "Epoch 29/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0314\n",
      "Epoch 30/1000\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0303\n",
      "Epoch 31/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0292\n",
      "Epoch 32/1000\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0282\n",
      "Epoch 33/1000\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0273\n",
      "Epoch 34/1000\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0265\n",
      "Epoch 35/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0257\n",
      "Epoch 36/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0250\n",
      "Epoch 37/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0243\n",
      "Epoch 38/1000\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0236\n",
      "Epoch 39/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0230\n",
      "Epoch 40/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0225\n",
      "Epoch 41/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0219\n",
      "Epoch 42/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0214\n",
      "Epoch 43/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0209\n",
      "Epoch 44/1000\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0204\n",
      "Epoch 45/1000\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0199\n",
      "Epoch 46/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0195\n",
      "Epoch 47/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0190\n",
      "Epoch 48/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0185\n",
      "Epoch 49/1000\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0181\n",
      "Epoch 50/1000\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 0.0177\n",
      "Epoch 51/1000\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0173\n",
      "Epoch 52/1000\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0169\n",
      "Epoch 53/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0165\n",
      "Epoch 54/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0161\n",
      "Epoch 55/1000\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.0157\n",
      "Epoch 56/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0153\n",
      "Epoch 57/1000\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.0149\n",
      "Epoch 58/1000\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 0.0145\n",
      "Epoch 59/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0141\n",
      "Epoch 60/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0138\n",
      "Epoch 61/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0134\n",
      "Epoch 62/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0131\n",
      "Epoch 63/1000\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0126\n",
      "Epoch 64/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0123\n",
      "Epoch 65/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0119\n",
      "Epoch 66/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0115\n",
      "Epoch 67/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0112\n",
      "Epoch 68/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0109\n",
      "Epoch 69/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0105\n",
      "Epoch 70/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0101\n",
      "Epoch 71/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0097\n",
      "Epoch 72/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0093\n",
      "Epoch 73/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0090\n",
      "Epoch 74/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0086\n",
      "Epoch 75/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0082\n",
      "Epoch 76/1000\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.0079\n",
      "Epoch 77/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0075\n",
      "Epoch 78/1000\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0072\n",
      "Epoch 79/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0068\n",
      "Epoch 80/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0064\n",
      "Epoch 81/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0061\n",
      "Epoch 82/1000\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.0057\n",
      "Epoch 83/1000\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.0054\n",
      "Epoch 84/1000\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0050\n",
      "Epoch 85/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0047\n",
      "Epoch 86/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0044\n",
      "Epoch 87/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0041\n",
      "Epoch 88/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0038\n",
      "Epoch 89/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0035\n",
      "Epoch 90/1000\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0032\n",
      "Epoch 91/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0030\n",
      "Epoch 92/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0028\n",
      "Epoch 93/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0026\n",
      "Epoch 94/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0024\n",
      "Epoch 95/1000\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0022\n",
      "Epoch 96/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0021\n",
      "Epoch 97/1000\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.0020\n",
      "Epoch 98/1000\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0018\n",
      "Epoch 99/1000\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0017\n",
      "Epoch 100/1000\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.001 - 0s 18ms/step - loss: 0.0017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0016\n",
      "Epoch 102/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0015\n",
      "Epoch 103/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0015\n",
      "Epoch 104/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0015\n",
      "Epoch 105/1000\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0014\n",
      "Epoch 106/1000\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0014\n",
      "Epoch 107/1000\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0013\n",
      "Epoch 108/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0013\n",
      "Epoch 109/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0013\n",
      "Epoch 110/1000\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0013\n",
      "Epoch 111/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0013\n",
      "Epoch 112/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0012\n",
      "Epoch 113/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0012\n",
      "Epoch 114/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0012\n",
      "Epoch 115/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0012\n",
      "Epoch 116/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0012\n",
      "Epoch 117/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0012\n",
      "Epoch 118/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0012\n",
      "Epoch 119/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0012\n",
      "Epoch 120/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0012\n",
      "Epoch 121/1000\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0011\n",
      "Epoch 122/1000\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.0011\n",
      "Epoch 123/1000\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0011\n",
      "Epoch 124/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0011\n",
      "Epoch 125/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0011\n",
      "Epoch 126/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0012\n",
      "Epoch 127/1000\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 0.0011\n",
      "Epoch 128/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0011\n",
      "Epoch 129/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0011\n",
      "Epoch 130/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0010\n",
      "Epoch 131/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.0010\n",
      "Epoch 132/1000\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0010\n",
      "Epoch 133/1000\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0010\n",
      "Epoch 134/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.0010\n",
      "Epoch 135/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0010\n",
      "Epoch 136/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 9.9584e-04\n",
      "Epoch 137/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 9.8918e-04\n",
      "Epoch 138/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 9.8780e-04\n",
      "Epoch 139/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 9.7372e-04\n",
      "Epoch 140/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 9.7795e-04\n",
      "Epoch 141/1000\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 9.6935e-04\n",
      "Epoch 142/1000\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 9.6344e-04\n",
      "Epoch 143/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 9.4539e-04\n",
      "Epoch 144/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 9.3770e-04\n",
      "Epoch 145/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 9.3727e-04\n",
      "Epoch 146/1000\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 9.3000e-04\n",
      "Epoch 147/1000\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 9.2220e-04\n",
      "Epoch 148/1000\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 9.2912e-04\n",
      "Epoch 149/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 9.4450e-04\n",
      "Epoch 150/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 9.4298e-04\n",
      "Epoch 151/1000\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 8.9772e-04\n",
      "Epoch 152/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 8.8619e-04\n",
      "Epoch 153/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 8.9434e-04\n",
      "Epoch 154/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 9.2218e-04\n",
      "Epoch 155/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 8.8649e-04\n",
      "Epoch 156/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 8.6445e-04\n",
      "Epoch 157/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 8.9227e-04\n",
      "Epoch 158/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 8.5371e-04\n",
      "Epoch 159/1000\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 8.4712e-04\n",
      "Epoch 160/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 8.6709e-04\n",
      "Epoch 161/1000\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 8.3574e-04\n",
      "Epoch 162/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 8.3705e-04\n",
      "Epoch 163/1000\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 8.2260e-04\n",
      "Epoch 164/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 8.3286e-04\n",
      "Epoch 165/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 8.0383e-04\n",
      "Epoch 166/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 8.4063e-04\n",
      "Epoch 167/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 8.1423e-04\n",
      "Epoch 168/1000\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 8.0323e-04\n",
      "Epoch 169/1000\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 8.2907e-04\n",
      "Epoch 170/1000\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 8.0384e-04\n",
      "Epoch 171/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 7.7447e-04\n",
      "Epoch 172/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 7.8108e-04\n",
      "Epoch 173/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 7.6373e-04\n",
      "Epoch 174/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 7.8861e-04\n",
      "Epoch 175/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 7.7617e-04\n",
      "Epoch 176/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 7.6611e-04\n",
      "Epoch 177/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 7.4842e-04\n",
      "Epoch 178/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 7.5340e-04\n",
      "Epoch 179/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 7.9272e-04\n",
      "Epoch 180/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 7.6856e-04\n",
      "Epoch 181/1000\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 7.5124e-04\n",
      "Epoch 182/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 7.2278e-04\n",
      "Epoch 183/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 7.2200e-04\n",
      "Epoch 184/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 7.1626e-04\n",
      "Epoch 185/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 7.1579e-04\n",
      "Epoch 186/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 7.1023e-04\n",
      "Epoch 187/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 7.0148e-04\n",
      "Epoch 188/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 7.0021e-04\n",
      "Epoch 189/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 7.0653e-04\n",
      "Epoch 190/1000\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 6.9169e-04\n",
      "Epoch 191/1000\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 6.8955e-04\n",
      "Epoch 192/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 6.9078e-04\n",
      "Epoch 193/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 6.7978e-04\n",
      "Epoch 194/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 6.8700e-04\n",
      "Epoch 195/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 6.7582e-04\n",
      "Epoch 196/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 6.6728e-04\n",
      "Epoch 197/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 6.6237e-04\n",
      "Epoch 198/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 6.6095e-04\n",
      "Epoch 199/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 6.5184e-04\n",
      "Epoch 200/1000\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 6.5153e-04\n",
      "Epoch 201/1000\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 6.4988e-04\n",
      "Epoch 202/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 6.4187e-04\n",
      "Epoch 203/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 6.4055e-04\n",
      "Epoch 204/1000\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 6.3780e-04\n",
      "Epoch 205/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 6.4477e-04\n",
      "Epoch 206/1000\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 6.3026e-04\n",
      "Epoch 207/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 6.3455e-04\n",
      "Epoch 208/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 6.2830e-04\n",
      "Epoch 209/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 6.2828e-04\n",
      "Epoch 210/1000\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 6.1611e-04\n",
      "Epoch 211/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 6.1056e-04\n",
      "Epoch 212/1000\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 6.1355e-04\n",
      "Epoch 213/1000\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 6.1858e-04\n",
      "Epoch 214/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 6.0201e-04\n",
      "Epoch 215/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 5.9177e-04\n",
      "Epoch 216/1000\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 5.9346e-04\n",
      "Epoch 217/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 6.1814e-04\n",
      "Epoch 218/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 7.1807e-04\n",
      "Epoch 219/1000\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 7.4515e-04\n",
      "Epoch 220/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 6.6604e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15,) (1, 1) 0 15\n",
      "(16,) (1, 1) 1 16\n",
      "(17,) (1, 1) 2 17\n",
      "(18,) (1, 1) 3 18\n",
      "(19,) (1, 1) 4 19\n",
      "(20,) (1, 1) 5 20\n",
      "(21,) (1, 1) 6 21\n",
      "(22,) (1, 1) 7 22\n",
      "(23,) (1, 1) 8 23\n",
      "(24,) (1, 1) 9 24\n",
      "(25,) (1, 1) 10 25\n",
      "(26,) (1, 1) 11 26\n",
      "(27,) (1, 1) 12 27\n",
      "(28,) (1, 1) 13 28\n",
      "(29,) (1, 1) 14 29\n",
      "(30,) (1, 1) 15 30\n",
      "(31,) (1, 1) 16 31\n",
      "(32,) (1, 1) 17 32\n",
      "(33,) (1, 1) 18 33\n",
      "(34,) (1, 1) 19 34\n",
      "(35,) (1, 1) 20 35\n",
      "(36,) (1, 1) 21 36\n",
      "(37,) (1, 1) 22 37\n",
      "(38,) (1, 1) 23 38\n",
      "(39,) (1, 1) 24 39\n",
      "(40,) (1, 1) 25 40\n",
      "(41,) (1, 1) 26 41\n",
      "(42,) (1, 1) 27 42\n",
      "(43,) (1, 1) 28 43\n",
      "(44,) (1, 1) 29 44\n",
      "(45,) (1, 1) 30 45\n",
      "(46,) (1, 1) 31 46\n",
      "(47,) (1, 1) 32 47\n",
      "(48,) (1, 1) 33 48\n",
      "(49,) (1, 1) 34 49\n",
      "(50,) (1, 1) 35 50\n",
      "(51,) (1, 1) 36 51\n",
      "(52,) (1, 1) 37 52\n",
      "(53,) (1, 1) 38 53\n",
      "(54,) (1, 1) 39 54\n",
      "(55,) (1, 1) 40 55\n",
      "(56,) (1, 1) 41 56\n",
      "(57,) (1, 1) 42 57\n",
      "(58,) (1, 1) 43 58\n",
      "(59,) (1, 1) 44 59\n",
      "(60,) (1, 1) 45 60\n",
      "(61,) (1, 1) 46 61\n",
      "(62,) (1, 1) 47 62\n",
      "(63,) (1, 1) 48 63\n",
      "(64,) (1, 1) 49 64\n",
      "(65,) (1, 1) 50 65\n",
      "(66,) (1, 1) 51 66\n",
      "(67,) (1, 1) 52 67\n",
      "(68,) (1, 1) 53 68\n",
      "(69,) (1, 1) 54 69\n",
      "(70,) (1, 1) 55 70\n",
      "(71,) (1, 1) 56 71\n",
      "(72,) (1, 1) 57 72\n",
      "(73,) (1, 1) 58 73\n",
      "(74,) (1, 1) 59 74\n",
      "(75,) (1, 1) 60 75\n",
      "(76,) (1, 1) 61 76\n",
      "(77,) (1, 1) 62 77\n",
      "(78,) (1, 1) 63 78\n",
      "(79,) (1, 1) 64 79\n",
      "(80,) (1, 1) 65 80\n",
      "(81,) (1, 1) 66 81\n",
      "(82,) (1, 1) 67 82\n",
      "(83,) (1, 1) 68 83\n",
      "(84,) (1, 1) 69 84\n",
      "(85,) (1, 1) 70 85\n",
      "(86,) (1, 1) 71 86\n",
      "(87,) (1, 1) 72 87\n",
      "(88,) (1, 1) 73 88\n",
      "(89,) (1, 1) 74 89\n",
      "(90,) (1, 1) 75 90\n",
      "(91,) (1, 1) 76 91\n",
      "(92,) (1, 1) 77 92\n",
      "(93,) (1, 1) 78 93\n",
      "(94,) (1, 1) 79 94\n",
      "(95,) (1, 1) 80 95\n",
      "(96,) (1, 1) 81 96\n",
      "(97,) (1, 1) 82 97\n",
      "(98,) (1, 1) 83 98\n",
      "(99,) (1, 1) 84 99\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, LSTM, SimpleRNN\n",
    "\n",
    "# time step만큼 시퀀스 데이터 분리\n",
    "def split_sequence(sequence, step):\n",
    "    x, y = list(), list()\n",
    "\n",
    "    for i in range(len(sequence)):\n",
    "        end_idx = i + step\n",
    "        if end_idx > len(sequence) - 1:\n",
    "            break\n",
    "\n",
    "        seq_x, seq_y = sequence[i:end_idx], sequence[end_idx]\n",
    "        x.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "\n",
    "# sin 함수 학습 데이터\n",
    "x = [i for i in np.arange(start=-10, stop=10, step=0.1)]\n",
    "train_y = [np.sin(i) for i in x]\n",
    "\n",
    "# 하이퍼파라미터\n",
    "n_timesteps = 15\n",
    "n_features = 1\n",
    "\n",
    "# 시퀀스 나누기\n",
    "# train_x.shape => (samples, timesteps)\n",
    "# train_y.shape => (samples)\n",
    "train_x, train_y = split_sequence(train_y, step=n_timesteps)\n",
    "print(\"shape x:{} / y:{}\".format(train_x.shape, train_y.shape))\n",
    "\n",
    "# RNN 입력 벡터 크기를 맞추기 위해 벡터 차원 크기 변경\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "train_x = train_x.reshape(train_x.shape[0], train_x.shape[1], n_features)\n",
    "print(\"train_x.shape = {}\".format(train_x.shape))\n",
    "print(\"train_y.shape = {}\".format(train_y.shape))\n",
    "\n",
    "# RNN 모델 정의\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=10, return_sequences=False, input_shape=(n_timesteps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# 모델 학습\n",
    "np.random.seed(0)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5, mode='auto')\n",
    "history = model.fit(train_x, train_y, epochs=1000, callbacks=[early_stopping])\n",
    "\n",
    "# loss 그래프 생성\n",
    "plt.plot(history.history['loss'], label=\"loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "# 테스트 데이터셋 생성\n",
    "test_x = np.arange(10, 20, 0.1)\n",
    "calc_y = np.cos(test_x) # 테스트 정답 데이터\n",
    "\n",
    "# RNN 모델 예측 및 로그 저장\n",
    "test_y = calc_y[:n_timesteps]\n",
    "for i in range(len(test_x) - n_timesteps):\n",
    "    net_input = test_y[i : i + n_timesteps]\n",
    "    net_input = net_input.reshape((1, n_timesteps, n_features))\n",
    "    train_y = model.predict(net_input, verbose=0)\n",
    "    print(test_y.shape, train_y.shape, i, i + n_timesteps)\n",
    "    test_y = np.append(test_y, train_y)\n",
    "\n",
    "# 예측 결과 그래프 그리기\n",
    "plt.plot(test_x, calc_y, label=\"ground truth\", color=\"orange\")\n",
    "plt.plot(test_x, test_y, label=\"predicitons\", color=\"blue\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylim(-2, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 6-6 곡선 예측 LSTM 모델 사용 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape x:(185, 15) / y:(185,)\n",
      "train_x.shape = (185, 15, 1)\n",
      "train_y.shape = (185,)\n",
      "Epoch 1/1000\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 0.5403\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.4801\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 0.4264\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.3784\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.3360\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 0.3031\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 0.2732 0s - loss: 0.\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 0.2500\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.2297\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.2126\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 0.1984\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1849\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1738\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.1629\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.1529\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.1432\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.1344\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.1256\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.1176\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1096\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.1023\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0952\n",
      "Epoch 23/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0882\n",
      "Epoch 24/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0818\n",
      "Epoch 25/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0756\n",
      "Epoch 26/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0693\n",
      "Epoch 27/1000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 0.0634\n",
      "Epoch 28/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0578\n",
      "Epoch 29/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0525\n",
      "Epoch 30/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0471\n",
      "Epoch 31/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0423\n",
      "Epoch 32/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0377\n",
      "Epoch 33/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0333\n",
      "Epoch 34/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0293\n",
      "Epoch 35/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0255\n",
      "Epoch 36/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0222\n",
      "Epoch 37/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0192\n",
      "Epoch 38/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0166\n",
      "Epoch 39/1000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0144\n",
      "Epoch 40/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0125\n",
      "Epoch 41/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0110\n",
      "Epoch 42/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0098\n",
      "Epoch 43/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0088\n",
      "Epoch 44/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0080\n",
      "Epoch 45/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0073\n",
      "Epoch 46/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0069\n",
      "Epoch 47/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0064\n",
      "Epoch 48/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0060\n",
      "Epoch 49/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0057\n",
      "Epoch 50/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0054\n",
      "Epoch 51/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0052\n",
      "Epoch 52/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0049\n",
      "Epoch 53/1000\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.0047\n",
      "Epoch 54/1000\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0045\n",
      "Epoch 55/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0043\n",
      "Epoch 56/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0041\n",
      "Epoch 57/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0040\n",
      "Epoch 58/1000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0038\n",
      "Epoch 59/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0036\n",
      "Epoch 60/1000\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 0.0035\n",
      "Epoch 61/1000\n",
      "6/6 [==============================] - 0s 32ms/step - loss: 0.0034\n",
      "Epoch 62/1000\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.0032\n",
      "Epoch 63/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0031: 0s - loss: 0.0\n",
      "Epoch 64/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0030\n",
      "Epoch 65/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0029\n",
      "Epoch 66/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0028\n",
      "Epoch 67/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0027\n",
      "Epoch 68/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0026\n",
      "Epoch 69/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0025\n",
      "Epoch 70/1000\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 0.0024\n",
      "Epoch 71/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0024\n",
      "Epoch 72/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0023\n",
      "Epoch 73/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0022\n",
      "Epoch 74/1000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 0.0022\n",
      "Epoch 75/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0021\n",
      "Epoch 76/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0020\n",
      "Epoch 77/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0020\n",
      "Epoch 78/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 0.0019\n",
      "Epoch 79/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0018\n",
      "Epoch 80/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0018\n",
      "Epoch 81/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0017\n",
      "Epoch 82/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0017\n",
      "Epoch 83/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0016\n",
      "Epoch 84/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 0.0016\n",
      "Epoch 85/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0016\n",
      "Epoch 86/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0015\n",
      "Epoch 87/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0015\n",
      "Epoch 88/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0014\n",
      "Epoch 89/1000\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 0.0014\n",
      "Epoch 90/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0014\n",
      "Epoch 91/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0013\n",
      "Epoch 92/1000\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0013\n",
      "Epoch 93/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0013\n",
      "Epoch 94/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0012\n",
      "Epoch 95/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0012\n",
      "Epoch 96/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0012\n",
      "Epoch 97/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0012\n",
      "Epoch 98/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0011\n",
      "Epoch 99/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0011\n",
      "Epoch 100/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 0.0011\n",
      "Epoch 101/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 0.0010\n",
      "Epoch 102/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.0010\n",
      "Epoch 103/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 9.8382e-04\n",
      "Epoch 104/1000\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 9.6205e-04\n",
      "Epoch 105/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 9.3684e-04\n",
      "Epoch 106/1000\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 9.1511e-04\n",
      "Epoch 107/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.9328e-04\n",
      "Epoch 108/1000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 8.7834e-04\n",
      "Epoch 109/1000\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 8.6806e-04\n",
      "Epoch 110/1000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 8.5438e-04\n",
      "Epoch 111/1000\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 8.1904e-04\n",
      "Epoch 112/1000\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 7.9377e-04\n",
      "Epoch 113/1000\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 7.7201e-04 0s - loss: 8.4222e\n",
      "Epoch 114/1000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 7.5619e-04\n",
      "Epoch 115/1000\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 7.4081e-04\n",
      "Epoch 116/1000\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 7.1796e-04\n",
      "Epoch 117/1000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 7.0528e-04\n",
      "Epoch 118/1000\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 6.8873e-04\n",
      "Epoch 119/1000\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 6.7061e-04\n",
      "Epoch 120/1000\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 6.6135e-04\n",
      "Epoch 121/1000\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 6.3921e-04\n",
      "Epoch 122/1000\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 6.2658e-04\n",
      "Epoch 123/1000\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 6.1204e-04\n",
      "Epoch 124/1000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 6.0194e-04\n",
      "Epoch 125/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 5.8429e-04\n",
      "Epoch 126/1000\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 5.6812e-04\n",
      "Epoch 127/1000\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 5.6539e-04\n",
      "Epoch 128/1000\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 5.4351e-04\n",
      "Epoch 129/1000\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 5.2672e-04\n",
      "Epoch 130/1000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 5.1485e-04\n",
      "Epoch 131/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 5.0110e-04\n",
      "Epoch 132/1000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 4.9390e-04\n",
      "Epoch 133/1000\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 4.7808e-04\n",
      "Epoch 134/1000\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 4.6885e-04\n",
      "Epoch 135/1000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 4.6023e-04\n",
      "Epoch 136/1000\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 4.4956e-04\n",
      "Epoch 137/1000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 4.3693e-04\n",
      "Epoch 138/1000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 4.2622e-04\n",
      "Epoch 139/1000\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 4.1623e-04\n",
      "Epoch 140/1000\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 4.0445e-04\n",
      "Epoch 141/1000\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 3.9345e-04\n",
      "Epoch 142/1000\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 3.8836e-04\n",
      "Epoch 143/1000\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 3.7433e-04\n",
      "Epoch 144/1000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 3.6613e-04\n",
      "Epoch 145/1000\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 3.5808e-04\n",
      "Epoch 146/1000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 3.4813e-04 0s - loss: 2.9616\n",
      "Epoch 147/1000\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 3.4001e-04\n",
      "Epoch 148/1000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 3.3149e-04\n",
      "Epoch 149/1000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 3.2262e-04 0s - loss: 2.9078\n",
      "Epoch 150/1000\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 3.1667e-04\n",
      "Epoch 151/1000\n",
      "6/6 [==============================] - 1s 132ms/step - loss: 3.0912e-04\n",
      "Epoch 152/1000\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 3.0096e-04\n",
      "Epoch 153/1000\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 2.9430e-04\n",
      "Epoch 154/1000\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 2.9137e-04\n",
      "Epoch 155/1000\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 2.7778e-04\n",
      "Epoch 156/1000\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 2.7374e-04\n",
      "Epoch 157/1000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 2.6610e-04\n",
      "Epoch 158/1000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 2.5990e-04\n",
      "Epoch 159/1000\n",
      "6/6 [==============================] - 1s 134ms/step - loss: 2.5207e-04\n",
      "Epoch 160/1000\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 2.4620e-04\n",
      "Epoch 161/1000\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 2.3975e-04\n",
      "Epoch 162/1000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 2.3436e-04\n",
      "Epoch 163/1000\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 2.2915e-04\n",
      "Epoch 164/1000\n",
      "6/6 [==============================] - 1s 133ms/step - loss: 2.2350e-04\n",
      "Epoch 165/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 2.1985e-04\n",
      "Epoch 166/1000\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 2.1297e-04\n",
      "Epoch 167/1000\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 2.0846e-04\n",
      "Epoch 168/1000\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 2.0543e-04\n",
      "Epoch 169/1000\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 1.9748e-04\n",
      "Epoch 170/1000\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 1.9505e-04\n",
      "Epoch 171/1000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 1.8786e-04\n",
      "Epoch 172/1000\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 1.8440e-04\n",
      "Epoch 173/1000\n",
      "6/6 [==============================] - 1s 115ms/step - loss: 1.8242e-04\n",
      "Epoch 174/1000\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 1.7575e-04\n",
      "Epoch 175/1000\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 1.7018e-04\n",
      "Epoch 176/1000\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 1.6822e-04\n",
      "Epoch 177/1000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 1.6319e-04\n",
      "Epoch 178/1000\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 1.6142e-04\n",
      "Epoch 179/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 1.5446e-04\n",
      "Epoch 180/1000\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 1.5601e-04\n",
      "Epoch 181/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 1.4843e-04\n",
      "Epoch 182/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 1.4749e-04\n",
      "Epoch 183/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.4340e-04\n",
      "Epoch 184/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.3848e-04\n",
      "Epoch 185/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.3797e-04\n",
      "Epoch 186/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.3496e-04\n",
      "Epoch 187/1000\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 1.3048e-04\n",
      "Epoch 188/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2805e-04\n",
      "Epoch 189/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.2569e-04\n",
      "Epoch 190/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.2559e-04\n",
      "Epoch 191/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.2225e-04\n",
      "Epoch 192/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1741e-04\n",
      "Epoch 193/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.1537e-04\n",
      "Epoch 194/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 36ms/step - loss: 1.1333e-04\n",
      "Epoch 195/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.0953e-04\n",
      "Epoch 196/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.1269e-04\n",
      "Epoch 197/1000\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 1.0708e-04\n",
      "Epoch 198/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.0631e-04\n",
      "Epoch 199/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.0191e-04\n",
      "Epoch 200/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.0007e-04\n",
      "Epoch 201/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 9.6606e-05\n",
      "Epoch 202/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 9.6823e-05\n",
      "Epoch 203/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 9.3947e-05\n",
      "Epoch 204/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 9.1294e-05\n",
      "Epoch 205/1000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 9.0253e-05\n",
      "Epoch 206/1000\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 8.7924e-05\n",
      "Epoch 207/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 8.6657e-05\n",
      "Epoch 208/1000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 8.4766e-05\n",
      "Epoch 209/1000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 8.3343e-05\n",
      "Epoch 210/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 8.2170e-05\n",
      "Epoch 211/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 8.1161e-05\n",
      "Epoch 212/1000\n",
      "6/6 [==============================] - 0s 51ms/step - loss: 8.3102e-05\n",
      "Epoch 213/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 7.6045e-05\n",
      "Epoch 214/1000\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 7.8090e-05\n",
      "Epoch 215/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 7.3955e-05\n",
      "Epoch 216/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 7.5203e-05\n",
      "Epoch 217/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 7.3102e-05\n",
      "Epoch 218/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 7.2229e-05\n",
      "Epoch 219/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 7.0049e-05\n",
      "Epoch 220/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 6.8768e-05\n",
      "Epoch 221/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 6.7389e-05\n",
      "Epoch 222/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 6.7160e-05\n",
      "Epoch 223/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 6.6025e-05\n",
      "Epoch 224/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 6.5779e-05\n",
      "Epoch 225/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 6.4160e-05\n",
      "Epoch 226/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 6.3383e-05\n",
      "Epoch 227/1000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 6.0899e-05\n",
      "Epoch 228/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 6.2255e-05\n",
      "Epoch 229/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 5.9552e-05\n",
      "Epoch 230/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 6.1200e-05\n",
      "Epoch 231/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 6.0002e-05\n",
      "Epoch 232/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 5.7044e-05\n",
      "Epoch 233/1000\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 5.7347e-05\n",
      "Epoch 234/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 5.5507e-05\n",
      "Epoch 235/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 5.4601e-05\n",
      "Epoch 236/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 5.4023e-05\n",
      "Epoch 237/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 5.3039e-05\n",
      "Epoch 238/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 5.2436e-05\n",
      "Epoch 239/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 5.1247e-05\n",
      "Epoch 240/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 5.1447e-05\n",
      "Epoch 241/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 5.0062e-05\n",
      "Epoch 242/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 5.0070e-05\n",
      "Epoch 243/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 4.8600e-05\n",
      "Epoch 244/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 4.8089e-05\n",
      "Epoch 245/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 4.7835e-05\n",
      "Epoch 246/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 4.9266e-05\n",
      "Epoch 247/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 4.7608e-05\n",
      "Epoch 248/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 4.5495e-05\n",
      "Epoch 249/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 4.5630e-05\n",
      "Epoch 250/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 4.6081e-05\n",
      "Epoch 251/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 4.3246e-05\n",
      "Epoch 252/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 4.4009e-05\n",
      "Epoch 253/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 4.3199e-05\n",
      "Epoch 254/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 4.2315e-05\n",
      "Epoch 255/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 4.2161e-05\n",
      "Epoch 256/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 4.1777e-05\n",
      "Epoch 257/1000\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 4.0503e-05\n",
      "Epoch 258/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 4.0558e-05\n",
      "Epoch 259/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 3.9770e-05\n",
      "Epoch 260/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 3.9612e-05\n",
      "Epoch 261/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 3.8922e-05\n",
      "Epoch 262/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 3.9067e-05\n",
      "Epoch 263/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 3.9139e-05\n",
      "Epoch 264/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 3.9803e-05\n",
      "Epoch 265/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 3.7826e-05\n",
      "Epoch 266/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 3.6613e-05\n",
      "Epoch 267/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 3.7198e-05\n",
      "Epoch 268/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 3.7197e-05\n",
      "Epoch 269/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 3.5747e-05\n",
      "Epoch 270/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 3.6606e-05\n",
      "Epoch 271/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 3.6088e-05\n",
      "Epoch 272/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 3.6381e-05\n",
      "Epoch 273/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 3.4406e-05\n",
      "Epoch 274/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 3.4106e-05\n",
      "Epoch 275/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 3.3291e-05\n",
      "Epoch 276/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 3.2906e-05\n",
      "Epoch 277/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 3.2489e-05\n",
      "Epoch 278/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 3.2902e-05\n",
      "Epoch 279/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 3.1590e-05\n",
      "Epoch 280/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 3.1526e-05\n",
      "Epoch 281/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 3.1028e-05\n",
      "Epoch 282/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 3.1416e-05\n",
      "Epoch 283/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 3.1514e-05\n",
      "Epoch 284/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 3.0260e-05\n",
      "Epoch 285/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 3.0841e-05\n",
      "Epoch 286/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 3.0309e-05\n",
      "Epoch 287/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.9447e-05\n",
      "Epoch 288/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 3.2386e-05\n",
      "Epoch 289/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 3.0453e-05\n",
      "Epoch 290/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.9668e-05\n",
      "Epoch 291/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 2.9166e-05\n",
      "Epoch 292/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 2.8889e-05\n",
      "Epoch 293/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.8253e-05\n",
      "Epoch 294/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 2.8011e-05\n",
      "Epoch 295/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 2.7557e-05\n",
      "Epoch 296/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 2.7320e-05\n",
      "Epoch 297/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 2.7115e-05\n",
      "Epoch 298/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.7013e-05\n",
      "Epoch 299/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.7199e-05\n",
      "Epoch 300/1000\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.7142e-05\n",
      "Epoch 301/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.7232e-05\n",
      "Epoch 302/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.6319e-05\n",
      "Epoch 303/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.5925e-05\n",
      "Epoch 304/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.5475e-05\n",
      "Epoch 305/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 2.5279e-05\n",
      "Epoch 306/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 2.4966e-05\n",
      "Epoch 307/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.5200e-05\n",
      "Epoch 308/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.5082e-05\n",
      "Epoch 309/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.4231e-05\n",
      "Epoch 310/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.4840e-05\n",
      "Epoch 311/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.3930e-05\n",
      "Epoch 312/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.3931e-05\n",
      "Epoch 313/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 2.3782e-05\n",
      "Epoch 314/1000\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.3543e-05\n",
      "Epoch 315/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.3933e-05\n",
      "Epoch 316/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 2.3916e-05\n",
      "Epoch 317/1000\n",
      "6/6 [==============================] - 0s 37ms/step - loss: 2.3091e-05\n",
      "Epoch 318/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.2962e-05\n",
      "Epoch 319/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 2.2731e-05\n",
      "Epoch 320/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 2.2719e-05\n",
      "Epoch 321/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.2855e-05\n",
      "Epoch 322/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 2.2906e-05\n",
      "Epoch 323/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 2.2403e-05\n",
      "Epoch 324/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.2388e-05\n",
      "Epoch 325/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 2.2016e-05\n",
      "Epoch 326/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 2.2008e-05\n",
      "Epoch 327/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.2295e-05\n",
      "Epoch 328/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.1594e-05\n",
      "Epoch 329/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.1266e-05\n",
      "Epoch 330/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 2.1040e-05\n",
      "Epoch 331/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0914e-05\n",
      "Epoch 332/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0830e-05\n",
      "Epoch 333/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 2.1586e-05\n",
      "Epoch 334/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 2.1033e-05\n",
      "Epoch 335/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.1469e-05\n",
      "Epoch 336/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 2.0689e-05\n",
      "Epoch 337/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 2.0323e-05\n",
      "Epoch 338/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 2.0143e-05\n",
      "Epoch 339/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 2.0395e-05\n",
      "Epoch 340/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9799e-05\n",
      "Epoch 341/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.9864e-05\n",
      "Epoch 342/1000\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.9943e-05\n",
      "Epoch 343/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.9436e-05\n",
      "Epoch 344/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9253e-05\n",
      "Epoch 345/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.9231e-05\n",
      "Epoch 346/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.9110e-05\n",
      "Epoch 347/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.9603e-05\n",
      "Epoch 348/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.9882e-05\n",
      "Epoch 349/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.8986e-05\n",
      "Epoch 350/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 2.0017e-05\n",
      "Epoch 351/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.8939e-05\n",
      "Epoch 352/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.8662e-05\n",
      "Epoch 353/1000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.8734e-05\n",
      "Epoch 354/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.8541e-05\n",
      "Epoch 355/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.8388e-05\n",
      "Epoch 356/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.8518e-05\n",
      "Epoch 357/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.8113e-05\n",
      "Epoch 358/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.8068e-05\n",
      "Epoch 359/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.8039e-05\n",
      "Epoch 360/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.7882e-05\n",
      "Epoch 361/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.7756e-05\n",
      "Epoch 362/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.7639e-05\n",
      "Epoch 363/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.7648e-05\n",
      "Epoch 364/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.7877e-05\n",
      "Epoch 365/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.7622e-05\n",
      "Epoch 366/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.7609e-05\n",
      "Epoch 367/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.7405e-05\n",
      "Epoch 368/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.6911e-05\n",
      "Epoch 369/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.8137e-05\n",
      "Epoch 370/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.8425e-05\n",
      "Epoch 371/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.8356e-05\n",
      "Epoch 372/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.7260e-05\n",
      "Epoch 373/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.6800e-05\n",
      "Epoch 374/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.7436e-05\n",
      "Epoch 375/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.6954e-05\n",
      "Epoch 376/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.7274e-05\n",
      "Epoch 377/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.6408e-05\n",
      "Epoch 378/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.6291e-05\n",
      "Epoch 379/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.6622e-05\n",
      "Epoch 380/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.6462e-05\n",
      "Epoch 381/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.6694e-05\n",
      "Epoch 382/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.6171e-05\n",
      "Epoch 383/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.6486e-05\n",
      "Epoch 384/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 34ms/step - loss: 1.6653e-05\n",
      "Epoch 385/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.6095e-05\n",
      "Epoch 386/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.6000e-05\n",
      "Epoch 387/1000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.6379e-05\n",
      "Epoch 388/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.5942e-05\n",
      "Epoch 389/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.6186e-05\n",
      "Epoch 390/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.5958e-05\n",
      "Epoch 391/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.6170e-05\n",
      "Epoch 392/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.5519e-05\n",
      "Epoch 393/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.5562e-05\n",
      "Epoch 394/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.5427e-05\n",
      "Epoch 395/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.5192e-05\n",
      "Epoch 396/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.5496e-05\n",
      "Epoch 397/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.5645e-05\n",
      "Epoch 398/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.5509e-05\n",
      "Epoch 399/1000\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 1.4866e-05\n",
      "Epoch 400/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.5120e-05\n",
      "Epoch 401/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.5147e-05\n",
      "Epoch 402/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.4892e-05\n",
      "Epoch 403/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.4536e-05\n",
      "Epoch 404/1000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.4720e-05\n",
      "Epoch 405/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.4394e-05\n",
      "Epoch 406/1000\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 1.4618e-05\n",
      "Epoch 407/1000\n",
      "6/6 [==============================] - 0s 49ms/step - loss: 1.5381e-05\n",
      "Epoch 408/1000\n",
      "6/6 [==============================] - 0s 48ms/step - loss: 1.5388e-05\n",
      "Epoch 409/1000\n",
      "6/6 [==============================] - 0s 34ms/step - loss: 1.4720e-05\n",
      "Epoch 410/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.4640e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15,) (1, 1) 0 15\n",
      "(16,) (1, 1) 1 16\n",
      "(17,) (1, 1) 2 17\n",
      "(18,) (1, 1) 3 18\n",
      "(19,) (1, 1) 4 19\n",
      "(20,) (1, 1) 5 20\n",
      "(21,) (1, 1) 6 21\n",
      "(22,) (1, 1) 7 22\n",
      "(23,) (1, 1) 8 23\n",
      "(24,) (1, 1) 9 24\n",
      "(25,) (1, 1) 10 25\n",
      "(26,) (1, 1) 11 26\n",
      "(27,) (1, 1) 12 27\n",
      "(28,) (1, 1) 13 28\n",
      "(29,) (1, 1) 14 29\n",
      "(30,) (1, 1) 15 30\n",
      "(31,) (1, 1) 16 31\n",
      "(32,) (1, 1) 17 32\n",
      "(33,) (1, 1) 18 33\n",
      "(34,) (1, 1) 19 34\n",
      "(35,) (1, 1) 20 35\n",
      "(36,) (1, 1) 21 36\n",
      "(37,) (1, 1) 22 37\n",
      "(38,) (1, 1) 23 38\n",
      "(39,) (1, 1) 24 39\n",
      "(40,) (1, 1) 25 40\n",
      "(41,) (1, 1) 26 41\n",
      "(42,) (1, 1) 27 42\n",
      "(43,) (1, 1) 28 43\n",
      "(44,) (1, 1) 29 44\n",
      "(45,) (1, 1) 30 45\n",
      "(46,) (1, 1) 31 46\n",
      "(47,) (1, 1) 32 47\n",
      "(48,) (1, 1) 33 48\n",
      "(49,) (1, 1) 34 49\n",
      "(50,) (1, 1) 35 50\n",
      "(51,) (1, 1) 36 51\n",
      "(52,) (1, 1) 37 52\n",
      "(53,) (1, 1) 38 53\n",
      "(54,) (1, 1) 39 54\n",
      "(55,) (1, 1) 40 55\n",
      "(56,) (1, 1) 41 56\n",
      "(57,) (1, 1) 42 57\n",
      "(58,) (1, 1) 43 58\n",
      "(59,) (1, 1) 44 59\n",
      "(60,) (1, 1) 45 60\n",
      "(61,) (1, 1) 46 61\n",
      "(62,) (1, 1) 47 62\n",
      "(63,) (1, 1) 48 63\n",
      "(64,) (1, 1) 49 64\n",
      "(65,) (1, 1) 50 65\n",
      "(66,) (1, 1) 51 66\n",
      "(67,) (1, 1) 52 67\n",
      "(68,) (1, 1) 53 68\n",
      "(69,) (1, 1) 54 69\n",
      "(70,) (1, 1) 55 70\n",
      "(71,) (1, 1) 56 71\n",
      "(72,) (1, 1) 57 72\n",
      "(73,) (1, 1) 58 73\n",
      "(74,) (1, 1) 59 74\n",
      "(75,) (1, 1) 60 75\n",
      "(76,) (1, 1) 61 76\n",
      "(77,) (1, 1) 62 77\n",
      "(78,) (1, 1) 63 78\n",
      "(79,) (1, 1) 64 79\n",
      "(80,) (1, 1) 65 80\n",
      "(81,) (1, 1) 66 81\n",
      "(82,) (1, 1) 67 82\n",
      "(83,) (1, 1) 68 83\n",
      "(84,) (1, 1) 69 84\n",
      "(85,) (1, 1) 70 85\n",
      "(86,) (1, 1) 71 86\n",
      "(87,) (1, 1) 72 87\n",
      "(88,) (1, 1) 73 88\n",
      "(89,) (1, 1) 74 89\n",
      "(90,) (1, 1) 75 90\n",
      "(91,) (1, 1) 76 91\n",
      "(92,) (1, 1) 77 92\n",
      "(93,) (1, 1) 78 93\n",
      "(94,) (1, 1) 79 94\n",
      "(95,) (1, 1) 80 95\n",
      "(96,) (1, 1) 81 96\n",
      "(97,) (1, 1) 82 97\n",
      "(98,) (1, 1) 83 98\n",
      "(99,) (1, 1) 84 99\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, LSTM\n",
    "\n",
    "# time step만큼 시퀀스 데이터 분리\n",
    "def split_sequence(sequence, step):\n",
    "    x, y = list(), list()\n",
    "\n",
    "    for i in range(len(sequence)):\n",
    "        end_idx = i + step\n",
    "        if end_idx > len(sequence) - 1:\n",
    "            break\n",
    "\n",
    "        seq_x, seq_y = sequence[i:end_idx], sequence[end_idx]\n",
    "        x.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "\n",
    "# sin 함수 학습 데이터\n",
    "x = [i for i in np.arange(start=-10, stop=10, step=0.1)]\n",
    "train_y = [np.sin(i) for i in x]\n",
    "\n",
    "# 하이퍼파라미터\n",
    "n_timesteps = 15\n",
    "n_features = 1\n",
    "\n",
    "# 시퀀스 나누기\n",
    "# train_x.shape => (samples, timesteps)\n",
    "# train_y.shape => (samples)\n",
    "train_x, train_y = split_sequence(train_y, step=n_timesteps)\n",
    "print(\"shape x:{} / y:{}\".format(train_x.shape, train_y.shape))\n",
    "\n",
    "# RNN 입력 벡터 크기를 맞추기 위해 벡터 차원 크기 변경\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "train_x = train_x.reshape(train_x.shape[0], train_x.shape[1], n_features)\n",
    "print(\"train_x.shape = {}\".format(train_x.shape))\n",
    "print(\"train_y.shape = {}\".format(train_y.shape))\n",
    "\n",
    "# LSTM 모델 정의\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=10, return_sequences=False, input_shape=(n_timesteps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# 모델 학습\n",
    "np.random.seed(0)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5, mode='auto')\n",
    "history = model.fit(train_x, train_y, epochs=1000, callbacks=[early_stopping])\n",
    "\n",
    "# loss 그래프 생성\n",
    "plt.plot(history.history['loss'], label=\"loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "# 테스트 데이터셋 생성\n",
    "test_x = np.arange(10, 20, 0.1)\n",
    "calc_y = np.cos(test_x) # 테스트 정답 데이터\n",
    "\n",
    "# RNN 모델 예측 및 로그 저장\n",
    "test_y = calc_y[:n_timesteps]\n",
    "for i in range(len(test_x) - n_timesteps):\n",
    "    net_input = test_y[i : i + n_timesteps]\n",
    "    net_input = net_input.reshape((1, n_timesteps, n_features))\n",
    "    train_y = model.predict(net_input, verbose=0)\n",
    "    print(test_y.shape, train_y.shape, i, i + n_timesteps)\n",
    "    test_y = np.append(test_y, train_y)\n",
    "\n",
    "# 예측 결과 그래프 그리기\n",
    "plt.plot(test_x, calc_y, label=\"ground truth\", color=\"orange\")\n",
    "plt.plot(test_x, test_y, label=\"predicitons\", color=\"blue\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylim(-2, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.3 양방향 LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 6-7 양방향 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: 0.7041 - accuracy: 0.0000e+00\n",
      "1/1 - 0s - loss: 0.7013 - accuracy: 0.2500\n",
      "1/1 - 0s - loss: 0.7026 - accuracy: 0.0000e+00\n",
      "1/1 - 0s - loss: 0.6985 - accuracy: 0.2500\n",
      "1/1 - 0s - loss: 0.7033 - accuracy: 0.0000e+00\n",
      "1/1 - 0s - loss: 0.7011 - accuracy: 0.0000e+00\n",
      "1/1 - 0s - loss: 0.6986 - accuracy: 0.0000e+00\n",
      "1/1 - 0s - loss: 0.7010 - accuracy: 0.0000e+00\n",
      "1/1 - 0s - loss: 0.7021 - accuracy: 0.0000e+00\n",
      "1/1 - 0s - loss: 0.6899 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6970 - accuracy: 0.0000e+00\n",
      "1/1 - 0s - loss: 0.6963 - accuracy: 0.2500\n",
      "1/1 - 0s - loss: 0.6957 - accuracy: 0.2500\n",
      "1/1 - 0s - loss: 0.6921 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.6947 - accuracy: 0.2500\n",
      "1/1 - 0s - loss: 0.6909 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6916 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.6894 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6882 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6882 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6840 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6874 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6856 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6847 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6829 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6835 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6847 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6748 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6845 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6743 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6774 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6831 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6703 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6763 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6924 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.6725 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6685 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6703 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6611 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6652 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6704 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6582 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6642 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6540 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6636 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6581 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6679 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6511 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6489 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6581 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6472 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6451 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6539 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6695 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6472 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6451 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6437 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6322 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6335 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6278 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6257 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6332 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6183 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6658 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6267 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6231 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6216 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6319 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6247 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.5894 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6172 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6011 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.5784 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6689 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.5695 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.5854 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6275 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.5916 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.5429 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.5499 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.5618 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.5458 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6558 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.5853 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6090 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.5786 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6171 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.7537 - accuracy: 0.2500\n",
      "1/1 - 0s - loss: 0.5397 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.5547 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.5383 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.7147 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.5519 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4844 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.7850 - accuracy: 0.2500\n",
      "1/1 - 0s - loss: 0.4890 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.5866 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.5420 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4921 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.7089 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.5011 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.5855 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.5182 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.7491 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.4748 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.5016 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4447 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4924 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4952 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4597 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4744 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4317 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.5038 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4570 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6077 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4753 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4536 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4730 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4241 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4269 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4154 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3855 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4426 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4166 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.5202 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4613 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4121 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4103 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3548 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4193 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3283 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3288 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3832 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3511 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4018 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2997 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2783 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3235 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4103 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2834 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3444 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3443 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3310 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3222 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3238 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2564 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4663 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2778 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.5085 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.9469 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.4336 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3117 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3289 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2706 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4014 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2540 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2771 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2691 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.7757 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.2389 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3374 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.7876 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.2967 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1972 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3676 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2314 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.9428 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.2348 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3068 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3174 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4137 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1985 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4145 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3019 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3206 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2077 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3450 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3761 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2973 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.8983 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.5063 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3802 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6667 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.2389 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4857 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3889 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3884 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4025 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3758 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2307 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: 0.1846 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2579 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4096 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2807 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1752 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3345 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1855 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2153 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2123 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2590 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4017 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2080 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3730 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.8294 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.1851 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3244 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1412 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4236 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3133 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2950 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2135 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2236 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2144 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.5697 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2719 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2481 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1932 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1930 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2397 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3061 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3385 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1661 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1909 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3864 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2307 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2155 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1768 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2825 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2636 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2698 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2668 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4927 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1657 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 1.1020 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.2849 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2850 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2897 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2439 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2671 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2708 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2740 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1506 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2321 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2935 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2822 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1641 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2194 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2458 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1590 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2234 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2218 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2977 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2108 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2112 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2895 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2272 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1987 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1724 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1467 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2024 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1755 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2088 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.7003 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.7343 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.1941 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1423 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2675 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2511 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1872 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2018 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1481 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3985 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1665 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1693 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 1.5245 - accuracy: 0.2500\n",
      "1/1 - 0s - loss: 0.1999 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2176 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 1.7426 - accuracy: 0.2500\n",
      "1/1 - 0s - loss: 0.2128 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2222 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1798 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2363 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6197 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4319 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2673 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1980 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1994 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1562 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1963 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3676 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1575 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1704 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1743 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2093 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2756 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2374 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.5186 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1769 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2450 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3335 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3450 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4797 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4762 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1473 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1885 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2057 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2230 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1635 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2111 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2316 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1173 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3084 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2540 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3390 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2208 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3421 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4799 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1870 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1821 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1765 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2691 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4670 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2465 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1803 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1318 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2121 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1819 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1534 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2304 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1839 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1859 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1266 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1950 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1862 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1777 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1384 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4470 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1405 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1549 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2779 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1201 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.5612 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1602 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2736 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2219 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1585 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.9987 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.5805 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1639 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1639 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1759 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1256 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1547 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1824 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2489 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1385 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3045 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2058 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3078 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.5466 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1575 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1963 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2112 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2710 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2473 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1793 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 1.1189 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.2008 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.5412 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2549 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1617 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2278 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1878 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4806 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2758 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1812 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1415 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1409 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1409 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2665 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2453 - accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: 0.1908 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1501 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1961 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1530 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6348 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1693 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1996 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1468 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1368 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2193 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1615 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2037 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6095 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1725 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1783 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 1.0903 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.1616 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.5315 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1538 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1543 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2715 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1488 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2777 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4412 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1831 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2415 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1848 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1385 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1733 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1844 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4028 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1836 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2753 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1461 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2028 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3531 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1035 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1449 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1168 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1591 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3556 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.5004 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1675 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3182 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1533 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2572 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1390 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1600 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2181 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4307 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1566 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1860 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1303 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 1.3325 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.1171 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 1.0799 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.2053 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2192 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1385 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1542 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1028 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1724 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2964 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1567 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1659 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2238 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2002 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2011 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2791 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1671 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1192 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3392 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.7439 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.1149 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1524 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2590 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4856 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4266 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 1.3587 - accuracy: 0.2500\n",
      "1/1 - 0s - loss: 0.2807 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1374 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2009 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1264 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1013 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2350 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1781 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1757 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.9117 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.1459 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.7810 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.9288 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.1672 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1112 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2223 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1775 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1525 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6311 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2588 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3524 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2795 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2096 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1824 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1696 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1549 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1704 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1642 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2182 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1648 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.8351 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.1800 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1388 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2511 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.7700 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.1652 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2029 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.7852 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.2511 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2252 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2662 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1662 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2646 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1808 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1789 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1418 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2054 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1859 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4321 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1538 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1476 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1984 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1466 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1639 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4779 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1997 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1850 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1687 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2439 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2017 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2321 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4881 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1669 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2125 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1482 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1498 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2326 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1403 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2506 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1313 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1930 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1922 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2599 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1727 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1419 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 1.0230 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.2727 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2097 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1558 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1128 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3774 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1672 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1576 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1468 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1538 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2523 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1535 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1634 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1427 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2134 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2431 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1653 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1581 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1253 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0939 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2074 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1123 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1213 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0897 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1049 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1814 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1416 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1442 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1454 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1562 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2623 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1599 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2849 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.0718 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1719 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1054 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1389 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.8919 - accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: 0.1541 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3909 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1736 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1836 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1257 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1897 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1009 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1579 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1530 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1520 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 1.2207 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.5557 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2592 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1858 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2214 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2157 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2033 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2840 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1838 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1427 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1400 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1586 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1459 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1593 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0861 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1489 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1443 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1898 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.0923 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1647 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2500 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1567 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3279 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4894 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1437 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.8524 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.1491 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1486 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1276 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1715 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0617 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1624 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4482 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1441 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1442 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1991 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2339 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2190 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1596 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0932 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1288 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1104 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0775 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1903 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2408 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1468 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0722 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2051 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1382 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1599 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 1.1835 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.4454 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1557 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1790 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4845 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1048 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1493 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2050 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1935 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1353 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1034 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1204 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1719 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0998 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1137 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1833 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1694 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2025 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1642 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1589 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1612 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1676 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0853 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.8083 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.0748 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6632 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1459 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1874 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4690 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1441 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2018 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.0806 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0515 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1728 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1699 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1496 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1374 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1570 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0995 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1286 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1412 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0978 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1300 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1487 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0951 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1705 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1732 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4745 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1330 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1450 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1400 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1638 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2053 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0759 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1217 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1340 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1274 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1107 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3423 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2146 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.0869 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1598 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2244 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1052 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1257 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0805 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1057 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0845 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2191 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1132 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2270 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1091 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1840 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.0694 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2048 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1296 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1769 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1709 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2028 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4686 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1130 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.9478 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.1980 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2194 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.5480 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1302 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1957 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1061 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1044 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2133 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3743 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.5281 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.0851 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1748 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1827 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1236 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1907 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2157 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.8420 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.1631 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3430 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.6153 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.1678 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2247 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3161 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1785 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1822 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2185 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1049 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1440 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1765 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1474 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4494 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1504 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1310 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4961 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1794 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4840 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1321 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1498 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1349 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1173 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2132 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1632 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1427 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1242 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0675 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2243 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1359 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1154 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.7330 - accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: 0.0788 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4667 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1691 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1032 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0910 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.5710 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.0459 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1816 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1685 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1324 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1892 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1186 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0673 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0700 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1690 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0862 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6849 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.1375 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0565 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1440 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2650 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4875 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1884 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1356 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0900 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4315 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1072 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.8034 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.4395 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1356 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2607 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1990 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3454 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.9462 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.7689 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.0826 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0942 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3568 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2171 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2407 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1250 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1298 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1479 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2450 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2183 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1587 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2964 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1917 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4476 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1324 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3234 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.0818 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4395 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1631 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2064 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1946 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1336 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1639 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2057 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1090 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1103 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1935 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1209 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2010 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1860 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1610 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1312 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3881 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1601 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.5786 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.1671 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1378 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1699 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0536 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1342 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.5596 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1357 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1056 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4196 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1459 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1056 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0504 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1175 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0989 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.5440 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.0584 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1933 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1213 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0779 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4929 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1067 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3650 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.0813 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1235 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1165 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1633 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1795 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1700 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1340 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1457 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1079 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4585 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2438 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1298 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0641 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1015 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0356 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0319 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1416 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2151 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0699 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0992 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2469 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1681 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3660 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1988 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3789 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1287 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1509 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3743 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1016 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0872 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2461 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2486 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1180 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2088 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1475 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1326 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.5239 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.0688 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3801 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1051 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1295 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1644 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1347 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6970 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1563 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1261 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1891 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.4089 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2018 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.0720 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2223 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1000 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1508 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1333 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2873 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1650 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0720 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1942 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1140 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1237 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3401 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3550 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1666 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0852 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1886 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2606 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1440 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3739 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1599 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0857 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3195 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1130 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1831 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1133 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1266 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1632 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1131 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2546 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1808 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0966 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0406 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1276 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0446 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0459 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1471 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1211 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0672 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0924 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0449 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1548 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0628 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2026 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1282 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1523 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1241 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.5269 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1371 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6658 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.5510 - accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: 0.1651 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4570 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2243 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.0489 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1971 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1345 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0948 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2333 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.6604 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.0479 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0936 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1527 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2048 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1292 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1972 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1321 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3990 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1823 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4211 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.2027 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1523 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1594 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1325 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0624 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0589 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.9233 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.0295 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1226 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.5929 - accuracy: 0.5000\n",
      "1/1 - 0s - loss: 0.0476 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1254 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1491 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3841 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.0561 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1792 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1550 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.4151 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.0993 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.3755 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.3135 - accuracy: 0.7500\n",
      "1/1 - 0s - loss: 0.1553 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.2603 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1735 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.0624 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1455 - accuracy: 1.0000\n",
      "1/1 - 0s - loss: 0.1649 - accuracy: 1.0000\n",
      "WARNING:tensorflow:From <ipython-input-22-306f6df7c622>:43: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "실젯값 : [0] 예측값 :  [0]\n",
      "실젯값 : [1] 예측값 :  [0]\n",
      "실젯값 : [1] 예측값 :  [1]\n",
      "실젯값 : [1] 예측값 :  [1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, TimeDistributed\n",
    "\n",
    "\n",
    "# 시퀀스 생성\n",
    "def get_sequence(n_timesteps):\n",
    "    # 0~1 사이의 랜덤 시퀀스 생성\n",
    "    X = np.array([random() for _ in range(n_timesteps)])\n",
    "\n",
    "    # 클래스 분류 기준\n",
    "    limit = n_timesteps / 4.0\n",
    "\n",
    "    # 누적합 시퀀스에서 클래스 결정\n",
    "    # 누적합 항목이 limit보다 작은 경우 0, 아닌 경우 1로 분류\n",
    "    y = np.array([0 if x < limit else 1 for x in np.cumsum(X)])\n",
    "\n",
    "    # LSTM 입력을 위해 3차원 텐서 형태로 변경\n",
    "    X = X.reshape(1, n_timesteps, 1)\n",
    "    y = y.reshape(1, n_timesteps, 1)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 정의\n",
    "n_units = 20\n",
    "n_timesteps = 4\n",
    "\n",
    "# 양방향 LSTM 모델 정의\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(n_units, return_sequences=True, input_shape=(n_timesteps, 1))))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "# 에포크마다 학습 데이터를 생성해서 학습\n",
    "for epoch in range(1000):\n",
    "    X, y = get_sequence(n_timesteps)\n",
    "    model.fit(X, y, epochs=1, batch_size=1, verbose=2)\n",
    "\n",
    "# 모델 평가\n",
    "X, y = get_sequence(n_timesteps)\n",
    "yhat = model.predict_classes(X, verbose=0)\n",
    "for i in range(n_timesteps):\n",
    "    print('실젯값 :', y[0, i], '예측값 : ', yhat[0, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.4 개체명 인식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 6-8 양방향 LSTM을 이용한 NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 크기 : \n",
      " 3555\n",
      "0번째 샘플 문장 시퀀스 : \n",
      " ['한편', ',', 'AFC', '챔피언스', '리그', 'E', '조', '에', '속하', 'ㄴ', '포항', '역시', '대회', '8강', '진출', '이', '불투명', '하', '다', '.']\n",
      "0번째 샘플 bio 태그 : \n",
      " ['O', 'O', 'O', 'O', 'O', 'B_OG', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "샘플 문장 시퀀스 최대 길이 : 168\n",
      "샘플 문장 시퀀스 평균 길이 : 34.03909985935302\n",
      "BIO 태그 사전 크기 : 8\n",
      "단어 사전 크기 : 13834\n",
      "[183, 11, 4276, 884, 162, 931, 402, 10, 2608, 7, 1516, 608, 145, 1361, 414, 4, 6347, 2, 8, 3]\n",
      "[1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "학습 샘플 시퀀스 형상 :  (2844, 40)\n",
      "학습 샘플 레이블 형상 :  (2844, 40, 8)\n",
      "테스트 샘플 시퀀스 형상 :  (711, 40)\n",
      "테스트 샘플 레이블 형상 :  (711, 40, 8)\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - 125s 5s/step - loss: 0.5145 - accuracy: 0.8334\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 128s 6s/step - loss: 0.2330 - accuracy: 0.8969\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 128s 6s/step - loss: 0.1530 - accuracy: 0.9270\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 128s 6s/step - loss: 0.1065 - accuracy: 0.9515\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 127s 6s/step - loss: 0.0716 - accuracy: 0.9685\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 124s 5s/step - loss: 0.0506 - accuracy: 0.9772\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 122s 5s/step - loss: 0.0391 - accuracy: 0.9824\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 119s 5s/step - loss: 0.0332 - accuracy: 0.9852\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 121s 5s/step - loss: 0.0267 - accuracy: 0.9877\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 118s 5s/step - loss: 0.0207 - accuracy: 0.9907\n",
      "23/23 [==============================] - 8s 361ms/step - loss: 0.2056 - accuracy: 0.9394\n",
      "평가 결과 :  0.9394348859786987\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seqeval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-eb359c1edd5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;31m# F1 스코어 계산을 위해 사용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mseqeval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1-score: {:.1%}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seqeval'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 학습 파일 불러오기\n",
    "def read_file(file_name):\n",
    "    sents = []\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, l in enumerate(lines):\n",
    "            if l[0] == ';' and lines[idx + 1][0] == '$':\n",
    "                this_sent = []\n",
    "            elif l[0] == '$' and lines[idx - 1][0] == ';':\n",
    "                continue\n",
    "            elif l[0] == '\\n':\n",
    "                sents.append(this_sent)\n",
    "            else:\n",
    "                this_sent.append(tuple(l.split()))\n",
    "    return sents\n",
    "\n",
    "\n",
    "# 학습용 말뭉치 데이터를 불러옴\n",
    "corpus = read_file('train.txt')\n",
    "\n",
    "# 말뭉치 데이터에서 단어와 BIO 태그만 불러와 학습용 데이터셋 생성\n",
    "sentences, tags = [], []\n",
    "for t in corpus:\n",
    "    tagged_sentence = []\n",
    "    sentence, bio_tag = [], []\n",
    "    for w in t:\n",
    "        tagged_sentence.append((w[1], w[3]))\n",
    "        sentence.append(w[1])\n",
    "        bio_tag.append(w[3])\n",
    "\n",
    "    sentences.append(sentence)\n",
    "    tags.append(bio_tag)\n",
    "\n",
    "print(\"샘플 크기 : \\n\", len(sentences))\n",
    "print(\"0번째 샘플 문장 시퀀스 : \\n\", sentences[0])\n",
    "print(\"0번째 샘플 bio 태그 : \\n\", tags[0])\n",
    "print(\"샘플 문장 시퀀스 최대 길이 :\", max(len(l) for l in sentences))\n",
    "print(\"샘플 문장 시퀀스 평균 길이 :\", (sum(map(len, sentences))/len(sentences)))\n",
    "\n",
    "\n",
    "# 토크나이저 정의\n",
    "sent_tokenizer = preprocessing.text.Tokenizer(oov_token='OOV') # 첫 번째 인덱스에는 OOV 사용\n",
    "sent_tokenizer.fit_on_texts(sentences)\n",
    "tag_tokenizer = preprocessing.text.Tokenizer(lower=False) # 태그 정보는 lower= False 소문자로 변환하지 않는다.\n",
    "tag_tokenizer.fit_on_texts(tags)\n",
    "\n",
    "# 단어 사전 및 태그 사전 크기\n",
    "vocab_size = len(sent_tokenizer.word_index) + 1\n",
    "tag_size = len(tag_tokenizer.word_index) + 1\n",
    "print(\"BIO 태그 사전 크기 :\", tag_size)\n",
    "print(\"단어 사전 크기 :\", vocab_size)\n",
    "\n",
    "# 학습용 단어 시퀀스 생성\n",
    "x_train = sent_tokenizer.texts_to_sequences(sentences)\n",
    "y_train = tag_tokenizer.texts_to_sequences(tags)\n",
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "\n",
    "# index to word / index to NER 정의\n",
    "index_to_word = sent_tokenizer.index_word # 시퀀스 인덱스를 단어로 변환하기 위해 사용\n",
    "index_to_ner = tag_tokenizer.index_word # 시퀀스 인덱스를 NER로 변환하기 위해 사용\n",
    "index_to_ner[0] = 'PAD'\n",
    "\n",
    "# 시퀀스 패딩 처리\n",
    "max_len = 40\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, padding='post', maxlen=max_len)\n",
    "y_train = preprocessing.sequence.pad_sequences(y_train, padding='post', maxlen=max_len)\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 8:2 비율로 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=.2, random_state=0)\n",
    "\n",
    "# 출력 데이터를 원-핫 인코딩\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=tag_size)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=tag_size)\n",
    "\n",
    "print(\"학습 샘플 시퀀스 형상 : \", x_train.shape)\n",
    "print(\"학습 샘플 레이블 형상 : \", y_train.shape)\n",
    "print(\"테스트 샘플 시퀀스 형상 : \", x_test.shape)\n",
    "print(\"테스트 샘플 레이블 형상 : \", y_test.shape)\n",
    "\n",
    "# 모델 정의(Bi-LSTM)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=30, input_length=max_len, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25)))\n",
    "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.01), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=10)\n",
    "print(\"평가 결과 : \", model.evaluate(x_test, y_test)[1])\n",
    "\n",
    "\n",
    "# 시퀀스를 NER 태그로 변환\n",
    "def sequences_to_tag(sequences):\n",
    "    result = []\n",
    "    for sequence in sequences:\n",
    "        temp = []\n",
    "        for pred in sequence:\n",
    "            pred_index = np.argmax(pred)\n",
    "            temp.append(index_to_ner[pred_index].replace(\"PAD\", \"O\"))\n",
    "        result.append(temp)\n",
    "    return result\n",
    "\n",
    "\n",
    "# 테스트 데이터셋의 NER 예측\n",
    "y_predicted = model.predict(x_test) # (711, 40) => model => (711, 40, 8)\n",
    "pred_tags = sequences_to_tag(y_predicted) # 예측된 NER\n",
    "test_tags = sequences_to_tag(y_test) # 실제 NER\n",
    "\n",
    "# F1 스코어 계산을 위해 사용\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "print(classification_report(test_tags, pred_tags))\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\n",
    "\n",
    "\n",
    "# 새로운 유형의 문장 NER 예측\n",
    "word_to_index = sent_tokenizer.word_index\n",
    "new_sentence = '삼성전자 출시 스마트폰 오늘 애플 도전장 내밀다.'.split()\n",
    "new_x = []\n",
    "for w in new_sentence:\n",
    "    try:\n",
    "        new_x.append(word_to_index.get(w, 1))\n",
    "    except KeyError:\n",
    "        # 모르는 단어의 경우 OOV\n",
    "        new_x.append(word_to_index['OOV'])\n",
    "\n",
    "print(\"새로운 유형의 시퀀스 : \", new_x)\n",
    "new_padded_seqs = preprocessing.sequence.pad_sequences([new_x], padding=\"post\", value=0, maxlen=max_len)\n",
    "\n",
    "# NER 예측\n",
    "p = model.predict(np.array([new_padded_seqs[0]]))\n",
    "p = np.argmax(p, axis=-1) # 예측된 NER 인덱스값 추출\n",
    "print(\"{:10} {:5}\".format(\"단어\", \"예측된 NER\"))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for w, pred in zip(new_sentence, p[0]):\n",
    "    print(\"{:10} {:5}\".format(w, index_to_ner[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
