# 6장. 챗봇 엔진에 필요한 딥러닝 모델

## 6.1 케라스(Keras)
여러 딥러닝 프레임워크가 있지만, `케라스(Keras)`는 직관적이고 사용하기 쉽다는 장점이 있다.  
무엇보다 `빠른 개발`을 목적으로 두고, 모듈 구성이 간단하여 비전문가들도 상대적으로 쉽게 사용할 수 있다.  
`케라스`는 `신경망 모델`을 구축할 수 있는 `고수준 API 라이브러리`이다.  
최근에 `텐서플로우 2.0`에 기본 API로 채택되어 구글의 지원을 받고있다.  

### 6.1.1 인공 신경망
`인공 신경망`은 두뇌의 `뉴런`을 모방한 모델이다.  

입력: x0, x1, x2  
가중치 : w0, w1, w2  

뉴런 : ∑wi*xi + b | f  
출력 : f(∑wi*xi + b)

뉴런의 계산 과정을 보면 입력된 xi값들과 대응되는 뉴런의 가중치 wi값들을 각각 곱해서 모두 더해준다.  
그리고 `편향값 b`를 통해 결과값을 조정한다.  
이를 수학적으로 표현하면 간단한 1차 함수의 모양이다.  
```
y = (w0x0 + w1x1 + w2x2) + b
```
실제 뉴런은 특정 강도 이상일 때만 다음 뉴런으로 신호를 전달하는데, 인공 신경망에서는 뉴런의 처리 과정 중 `f`로 표시된 영역이다.  
이를 `활성화 함수`라고 하며, 가중치 계산 결과값 y가 최종적으로 어떤 형태의 출력값으로 내보낼지 결정한다.  
활성화 함수는 종류가 많은데, 여기서는 3가지만 다룬다.  
1. 스텝 함수
`스텝 함수`는 가장 기본이 되는 함수이다.  
그래프가 `계단`같이 생겨서 스텝 함수이고, 입력값이 0보다 크면 1로, 0 이하일 때는 0으로 만든다.  
즉 입력값이 양수일 때만 활성화 시킨다.  

2. 시그모이드 함수
이진 분류에서 스텝 함수의 문제를 해결하기 위해 사용한다.  
스텝 함수에서 판단의 기준이 되는 `임계치` 부근의 데이터를 고려하지 않는 문제를 해결하기 위해 계단 모양을 완만한 형태로 표현했다.  
수식 : S(t) = 1 / (1 + e^-t)

3. ReLU 함수
입력값이 0 이상인 경우에는 기울기가 1인 직선이고, 0보다 작을 때는 결과값이 0이다.  
`시그모이드 함수`에 비해 연산 비용이 크지 않아 학습 속도가 빠르다.  

실제로 문제를 신경망 모델로 해결할 때는 1개의 뉴런만 사용하는 것이 아니라, 문제가 복잡할 수록 뉴런의 수가 늘어나고 신경망의 계층도 깊어진다.  

입력층과 출력층으로만 구성되어 있는 단순한 신경망을 `단층 신경망`이라고 한다.  
입력층과 1개 이상의 은닉층, 출력층으로 구성되어 있는 신경망은 `심층 신경망`이라고 한다.  
흔히 `딥러닝`과 `신경망`이라고 부르는 것들이 바로 `심층 신경망`이다.  
*신경망 계층이 `깊게(deep)`구성되어 각각의 뉴런을 `학습(learning)`시킨다 해서 딥러닝*

주로 `입력층`을 구성하는 뉴런들은 1개의 입력값을 갖고, 가중치와 활성화 함수를 갖고 있지 않아 입력된 값을 그대로 출력하는 특징이 있다.  
`출력층`의 뉴런은 각각 1개의 출력값을 갖고 있고, 지정된 활성화 함수에 따른 출력 범위를 갖고 있다.  
"복잡한 문제일 수록 뉴런와 `은닉층` 수를 늘리면 좋다"라고 하지만, 계산해야 하는 파라미터가 많아지면 비용이 높아지는 단점이 있다.  

신경망에 대해서 알아보자.  
신경망 모델에서 입력층으로부터 출력층까지 데이터가 순방향으로 전파되는 과정을 `순전파`라고 한다.  
데이터가 순방형으로 전파될 때 현 단계 뉴런의 가중치와 전 단계 뉴런의 출력값의 곱을 입력값으로 받는다.  
이 값은 활성화 함수를 통해 다음 뉴런으로~  

[순전파 예시]  
입력층: x --(전파: w_h * x)--> 은닉층: h --(전파: w_y * h)--> 결과값(yout)  

결과값이 실제 값과 오차가 많다면?  
다음 순전파 진행 시 오차가 줄어드는 방향으로 가중치(w_h, w_y)를 역방향으로 갱신해나간다.  
이 과정을 `역전파`라고 한다.  

### 6.1.2 딥러닝 분류 모델 만들기
[History 객체]  
loss : 각 에포크마다의 학습 손실값  
accuracy : 각 에포크마다의 학습 정확도  
val_loss : 각 에포크마다의 검증 손실값  
val_accuracy : 각 에포크마다의 검증 정확도  
epoch(에포크) : 학습의 횟수  

```
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense

# MNIST 데이터셋 가져오기
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0 # 데이터 정규화

# tf.data를 사용하여 데이터셋을 섞고 배치 만들기
ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000)
train_size = int(len(x_train) * 0.7) # 학슴셋 : 검증셋 = 7 : 3
train_ds = ds.take(train_size).batch(20)
val_ds = ds.skip(train_size).batch(20)

# MNIST 분류 모델 구성
model = Sequential()
model.add(Flatten(input_shape=(28, 28)))
model.add(Dense(20, activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(10, activation='softmax'))

# 모델 생성
model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])
# model.complie(loss='categorial_crossentropy', optimizer='sgd', metrics['accuracy'])

# 모델 학습
hist = model.fit(train_ds, validation_data=val_ds, epochs=10)

# 모델 평가
print('모델 평가')
model.evaluate(x_test, y_test)

# 모델 정보 출력
model.summary()

# 모델 저장
model.save('mnist_model.h5')

# 학습 결과 그래프 그리기
fig, loss_ax = plt.subplots()
acc_ax = loss_ax.twinx()

loss_ax.plot(hist.history['loss'], 'y', label='train loss')
loss_ax.plot(hist.history['var_loss'], 'r', label='var loss')

acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')
acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')

loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
acc_ax.set_ylabel('accuracy')

loss_ax.legend(loc='upper left')
acc_ax.legend(loc='lower left')
plt.show()
```  
위 코드를 돌리다가 만약 주피터에서 이런 오류가 뜬다면??
> The kernel appears to have died. It will restart automatically.

이 오류가 발생하는 이유는 `커널이 죽어서`인데, 커널이 죽는 이유는 `메모리 할당량을 초과`했기 때문!  
[해결방법]  
1. `jupyter_notebook_config.py`에서 다음 코드를 입력하고 재부팅하기.    
    - 위의 .py파일은 `./jupyter`폴더 안에 있음  
> c.NotebookApp.max_buffer_size = 100000000000000000000000

이렇게 큰 값을 입력해 바꾸면 됨



## 6.2.1 CNN 모델 개념
`CNN`을 이해하기 전에 `합성곱`과 `풀링`연산이 무엇인지 알아야 한다.  

[합성곱 연산]    
`합성곱 연산`이란, `합성곱 필터`로 불리는 특정 크기의 행렬을 이미지 데이터(or 문장 데이터) 행렬에 슬라이딩하면서 곱하고 더하는 연산을 의미함.  
`합성곱 필터`란 경우에 따라 마스크, 윈도우, 커널 등으로 다양하게 불린다.  
*이 책에서는 필터 혹은 커널로 부름*

[풀링 연산]  
`풀링 연산`이란, 합성곱 연산 결과로 나온 특징맵의 크기를 줄이거나 주요한 특징을 추출하기 위해 사용하는 연산으로, `최대 풀링`과 `평균 풀링`이 있다.  
이 중에서는 `최대 풀링 연산`을 주로 사용한다고 한다.  
`풀링 연산`에서도 합성곱 연산에서 사용하는 윈도우 크기, 스트라이드, 패딩 개념이 동일하게 적용된다.  *풀링 연산에서는 필터보단 윈도우라는 용어를 사용한다.*



## 6.3 개체명 인식을 위한 양방향  LSTM 모델
챗봇 엔진에 `개체명 인식`을 위해  사용하는 `양방향 LSTM`에 대해서 학습한다.  
`LSTM`은 순환 신경망 모델의 일종으로 시퀀스 또는 시계열 데이터의 패턴을 인식하는 분야에서 많이 사용한다.  
연속적인 데이터의 패턴을 이용해 결과를 예측하므로, 주로 주가 예측이나 신호 분석 및 번역 분야에서 좋은 성능을 보인다.  

### 6.3.1 RNN
`LSTM`은 `RNN`모델에서 파생된 모델이다.  
그래서 `RNN`이 뭔데?  
`RNN`은 순환 신경망으로 불리며, 앞서 배운 신경망 모델과 다르게 은닉층 노드의 출력값을 출력층과 그 다음 시점의 은닉층 노드의 입력으로 전달해 순환하는 특징을 갖고 있다.  

`RNN` 모델은 어떤 문제를 해결하느냐에 따라 입력과 출력의 길이를 조절할 수 있다.  
여러 개를 입력받아 하나를 출력하는 모델 : `many-to-one 모델`  
한 개를 입력받아 여러 개를 출력하는 모델 : `one-to-many 모델`  
여러 개를 입력받아 여러 개를 출력하는 모델 : `many-to-many 모델`   

참고로, `RNN`은 모든 시점에서 동일한 가중치와 편향값을 사용한다.  


### 6.3.2 LSTM
`RNN`은 입력 시퀀스의 시점이 길어질 수록 앞쪽의 데이터가 뒤쪽으로 잘 전달되지 않아 학습능력이 떨어진다는 단점이 있다.  
또한 RNN을 다층 구조로 쌓으면 입력과 출력 데이터 사이의 연관 관계가 줄어들어 장기 의존성 문제가 발생한다.  
이런 문제를 보완하기 위해 RNN을 변형시켜 나온 것이 바로 `LSTM`!!  

`LSTM`의 내부 구조를 살펴보자.  
`LSTM`은 기본적인 RNN의 은닉상태를 계산하는 방식에 변화가 있으며, 은닉 상탯값 이외에 셀 상태값이 추가되었다.  
은닉 상태값과 셀 상태값을 계산하기 위해 3개의 `게이트(Gate)`가 추가되었다.  
1. 입력 게이트
    - 현재 정보를 얼마나 기억할지 결정하는 게이트
2. 삭제 게이트
    - 이전 시점의 셀 상태값을 삭제하기 위해 사용되는 게이트
3. 출력 게이트
    - 출력 게이트의 결과값은 현재 은닉 시점의 은닉 상태를 결정하는데 사용되며, 해당 값은 전달되는 메모리 셀이 많아질수록 정보 유실이 크기 때문에 `단기 상태`라고 부름


### 6.3.3 양방향 LSTM
`RNN`이나 `LSTM`은 일반 신경망과 다르게 시퀀스 또는 시계열 데이터 처리에 특화되어 은닉층에서 과거의 정보를 기억할 수 있다.  
하지만 `순환 신경망`의 특성상 데이터가 입력 순으로 처리되기 때문에 이전 시점의 정보만 활용할 수 밖에 없다는 단점이 존재하는데...  

[예시]  
> ios앱 "개발"은 맥북이 필요합니다.  

일반적인 `RNN`이나 `LSTM`에서는 'ios'와 '앱'이라는 단어만으로 " "에 "개발"이라는 단어가 들어갈 것이라고 유추하기에는 정보가 부족하다.  
따라서 자연어 처리에 있어서 입력 데이터의 정방향 처리만큼 역방향 처리도 중요하다!! 이말입니다~  

`양방향 LSTM`은 기존 `LSTM계층`에 역방향으로 처리하는 `LSTM계층`을 하나 더 추가해 양방향에서 문장의 패턴을 분석할 수 있도록 되어있다.  
이러면 입력 문장을 양방향에서 처리하므로 문장이 길어진다고 하더라도 정보 손실 없이 처리가 가능하다고 한다!  

### 6.3.4 개체명 인식
`개체명 인식(NER)`이란 문장에서 각 개체의 `유형`을 인식하는 동작이다.  
즉, 문장 내에 포함된 어떤 단어가 인물, 장소, 날짜 등을 의미하는 단어인지 인식하는 것이다.  
딥러닝 모델이나 확률 모델 등을 이용해 문장에서 개체명을 인식하는 프로그램을 `개체명 인식기`라고 부른다!  

`개체명 인식`은 챗봇에서 문장을 정확하게 해석하기 위해 "반드시" 해야하는 `전처리 과정`이다!!  

개체명 인식기를 개발하는 것은 매우 어려운 과정이다...  
국내에는 공개되어있는 학습 데이터도 많지 않을 뿐더러, 개발하기 위해 학습 데이터를 만드는 것 또한 많은 시간과 비용이 들어가기 떄문..ㅜㅜ  

그래서 이 책에서는 완벽한 개체명 인식 모델은 아니더라도 순환 신경망을 이용헤 개체명을 인식하는 원리를 익힐 수 있도록 한다.  
약간의 문제가 존재하지만, 학습 데이터만 충분하다면 토이 챗봇을 만드는 데에는 무리가 없다고 한다!  

개체명 인시기 모델을 만들기 위해서는 먼저 `BIO 표기법`을 알아야 한다.  

`BIO 표기법`이란, "Beginning Inside, Outside"의 약자로, 각 토큰마다 태그를 붙이기 위해 사용한다.  
`B`는 개체명이 시작되는 단어에 `B-개체명`으로 태그되며, `I`는 `B-개체명`과 연결되는 단어일 때 `I-개체명`으로 태그된다.  
`O`는 개체명 이외의 모든 토큰에 태그된다.  

[예시]  
```
오늘부터 샤닐 길동은 삼성전자에 근무합니다.  

오늘 / B-Date  
부터 / O  
  
샤닐 / B-Person  
길동 / I-Person
은 / O  
  
삼성 / B-Company  
전자 / I-Company  
에 / O  
  
근무 / O  
합니다 / O
```  
위의 예시와 같이 두 개 이상의 토큰이 하나의 개체를 구성하는 경우가 많기 때문에 `BIO 표기법`을 사용하는 것이다.  