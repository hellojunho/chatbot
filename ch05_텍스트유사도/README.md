# 5장. 텍스트 유사도
Word2Vec은 인공 신경망을 이용했다.  
이번엔 통계적인 방법을 이용해 유사도를 계산해보자.  

## 5.1. n-gram 유사도
`n-gram`은 주어진 문장에서 n개의 연속적인 단어 시퀀스를 의미한다.  
n-gram은 문장에서 n개의 단어를 토큰으로 사용한다.  
이 것은 `이웃한 단어`의 `출현 횟수`를 통계적으로 표현해 텍스트의 `유사도`를 계산하는 방법이다.  

[n에 따른 n-gram]  
```
'1661년' '6월' '뉴턴'은 '선생님'의 '제안'으로 '트리니티'에 '입학'하였다.  

n=1 (unigram) : 1661년/6월/뉴턴/선생님/제안/트리니티/입학  
n=2 (bigram) : 1661년 6월/6월 뉴턴/뉴턴 선생님/선생님 제안/... /트리니티 입학  
...
n=4 (4-gram) : 1661년 6월 뉴턴 선생님/6월 뉴턴 선생님 제안/뉴턴 선생님 제안 트리니티/../선생님 제안 트리니티 입학  
n=7 : 1661년 6월 뉴턴 선생님 제안 트리니티 입학
```
n=1 : unigram, n=2 : bigram, n=3 : trigram, n=4 : 4-gram ...

문장 A와 B가 있다고 하자.  
문장 B가 A와 얼마나 유사할까?  
similarity = tf(A, B)/tokens(A)  
- n-gram 유사도 공식
    - tf(A, b) : A와 B에서 동일한 토큰의 출현 빈도
    - tokens()는 해당 문장이 전체 토큰 수

[2-gram을 이용한 예시]  
A: 6월에 뉴턴은 선생님의 제안으로 트리니티에 입학했다.  
B: 6월에 뉴턴은 선생님의 제안으로 대학교에 입학했다.  

- 문장 A에서는 6개의 토큰이 전부 나온다.  
- 문장 B에서는 동일한 토큰이 4개 카운트 되었다.  
- 즉, `n-gram 유사도 수식`에 따라 4/6으로 `0.66`의 유사도를 지닌다. == "B는 A와 66% 유사합니다."  


## 5.2. 코사인 유사도
앞서, 단어나 문장을 벡터로 표현할 수 있었다.  
그렇다면 벡터 간의 거리나 각도를 이용해 유사성을 파악할 수도 있다.  
벡터 간 거리를 구하는 방법은 다양하지만, 여기서는 `코사인 유사도`를 사용할 것이다.  
`코사인 유사도`는 두 벡터 간 코사인 각도를 이용해 유사도를 측정하는 방법이다.  
이 유사도는 벡터의 크기가 중요하지 않을 때 그 거리를 측정하기 위해 사용된다.  

similarity = cos(세타) = A*B / |A||B|


[예시]  
A: 6월에 뉴턴은 선생님의 제안으로 트리니티에 입학했다.  
B: 6월에 뉴턴은 선생님의 제안으로 대학교에 입학했다.  
 
각 문장에 대하여 ['6월', '뉴턴', '선생님', '제안', '트리니티', '입학', '대학'] 의 토큰 값들은
A = [1, 1, 1, 1, 1, 1, 0]  
B = [1, 1, 1, 1, 0, 1, 1]  

[분자 계산]
A*B  = i=1 ~ n Ai * Bi의 합 = (1 * 1) + (1 * 1) + (1 * 1) + (1 * 1) + (1 * 0) + (1 * 1) + (0 * 1) = 1 + 1 + 1 + 1 + 0 + 1 + 0 = 5
 
[분모 계산]
|A||B| = (1^2 + 1^2 + 1^2 + 1^2 + 1^2 + 1^2 + 0^2)^1/2 * (1^2 + 1^2 + 1^2 + 1^2 + 0^2 + 1^2 + 1^2)^1/2 = 6^1/2 * 6^1/2 = 6

즉, 5/6이므로 두 문장은 `0.83333`만큼 유사도를 갖는다.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 