{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3장. 토크나이징\n",
    "컴퓨터가 자연어 의미를 분석해 컴퓨터가 처리할 수 있도록 하는 일을 `자연어 처리`라고 한다.  \n",
    "\n",
    "## 자연어 처리란\n",
    "`자연어 처리`란, `Natural Language Processing`의 약자로 `NLP`라고 부른다.  \n",
    "챗봇도 자연어 처리의 한 분야임!  \n",
    "\n",
    "어떤 방식으로 자연어를 컴퓨터에게 학습시킬까?  \n",
    "먼저 문장을 일정한 의미가 있는 `가장 작은 단어`들로 나누어, 그 단어들을 이용해 분석한다.  \n",
    "여기서 `가장 작은 단어`가 바로 `토큰(token)`이다!!  \n",
    "\n",
    "## 토크나이징\n",
    "위에서 `토큰`이 무엇인지 알아봤는데, `토크나이징`은 말 그대로 `토큰화`하는 과정이라고 이해했다!  \n",
    "여기서는 `KoNLPy(코엔엘파이)` 라이브러리를 사용했음!!  \n",
    "\n",
    "## 3.2. KoNLPy\n",
    "`KoNLPy`는 기본적인 한국어 자연어 처리를 위한 파이썬 라이브러리이다.  \n",
    "여기서는 한국어 문장을 토크나이징 작업을 제일 먼저 수행할건데, 토큰 단위를 `형태소`를 기본으로 하여 토큰화 할 것이다.  \n",
    "\n",
    "## 3.2.1 Kkma\n",
    "`Kkma`는 `꼬꼬마`라고 부름!  \n",
    "꼬꼬마 형태소 분석기를 사용하려면 `konlpy.tag`패키지의 `Kkma`모듈을 불러와야 함.  \n",
    "\n",
    "### Kkma 모듈 함수\n",
    "\n",
    "morphs(pharse) : 인자로 입력한 문장을 `형태소` 단위로 토크나이징함. 토크나이징된 형태소들은 `리스트` 형태로 반환.  \n",
    "nouns(pharse)  : 인자로 입력한 문장에서 `명사`인 토큰만 추출.  \n",
    "pos(pharse, flatten=True) : `POS tagge`라고 하며, 인자로 입력한 문장에서 형태소를 추출한 뒤, 품사 `태깅`을 함. `튜플`형태로 묶여서 `리스트`형태로 반환.  \n",
    "sentences(pharse) : 인자로 입력한 여러 문장을 분리해줌. `리스트` 형태로 반환.  \n",
    "\n",
    "\n",
    "[Kkma 품사 태그]  \n",
    "NNG : 일반 명사  \n",
    "JKS : 주격 조사  \n",
    "JKM : 부사격 조사  \n",
    "VV  : 동사  \n",
    "EFN : 평서형 종결 어미  \n",
    "SF  : 마침표, 물음표, 느낌표  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아버지', '가', '방', '에', '들어가', 'ㅂ니다', '.']\n",
      "[('아버지', 'NNG'), ('가', 'JKS'), ('방', 'NNG'), ('에', 'JKM'), ('들어가', 'VV'), ('ㅂ니다', 'EFN'), ('.', 'SF')]\n",
      "['아버지', '방']\n",
      "['오늘 날씨는 어 때요?', '내일은 덥다 던데.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "\n",
    "# 꼬꼬마 형태소 분석기 객체 생성\n",
    "kkma = Kkma()\n",
    "\n",
    "text = \"아버지가 방에 들어갑니다.\"\n",
    "\n",
    "# 형태소 추출\n",
    "morphs = kkma.morphs(text)\n",
    "print(morphs)\n",
    "\n",
    "# 형태소 품사 태그 추출\n",
    "pos = kkma.pos(text)\n",
    "print(pos)\n",
    "\n",
    "# 명사만 추출\n",
    "nouns = kkma.nouns(text)\n",
    "print(nouns)\n",
    "\n",
    "# 문장 분리\n",
    "sentences = \"오늘 날씨는 어때요? 내일은 덥다던데.\"\n",
    "s = kkma.sentences(sentences)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 Komoran\n",
    "`Komoran`은 자바로 개발한 한국어 형태소 분석기이다.  \n",
    "`코모란`이라고 부르고, 다른 분석기와는 다르게 `공백이 포함된 형태소`단위로도 분석 가능해 자주 쓰임!!  \n",
    "이걸 쓰려면 코모란 모듈을 불러와야 함!  \n",
    "\n",
    "```\n",
    "from konlpy.tag import Komoran\n",
    "```\n",
    "\n",
    "### Komoran 모듈의 함수 설명\n",
    "morphs(pharse) : 인자로 입력한 문장을 형태소 단위로 토크나이징함. `리스트`형태로 반환.  \n",
    "nouns(pharse)  : 인자로 입력한 문장에서 `명사`들만 추출.  \n",
    "pos(pharse, flatten=True) : `POS tagge`라고 하며, 인자로 입력한 문장에서 형태소를 추출한 뒤, 품사 `태깅`을 함. `튜플`형태로 묶여서 `리스트`형태로 반환. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아버지', '가', '방', '에', '들어가', 'ㅂ니다', '.']\n",
      "[('아버지', 'NNG'), ('가', 'JKS'), ('방', 'NNG'), ('에', 'JKB'), ('들어가', 'VV'), ('ㅂ니다', 'EF'), ('.', 'SF')]\n",
      "['아버지', '방']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "\n",
    "# 코모란 형태소 분석기 객체 생성\n",
    "komoran = Komoran()\n",
    "\n",
    "text = \"아버지가 방에 들어갑니다.\"\n",
    "\n",
    "# 형태소 추출\n",
    "morphs = komoran.morphs(text)\n",
    "print(morphs)\n",
    "\n",
    "# 형태소 품사 태그 추출\n",
    "pos = komoran.pos(text)\n",
    "print(pos)\n",
    "\n",
    "# 명사만 추출\n",
    "nouns = komoran.nouns(text)\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3 Okt\n",
    "`Okt`는 트위터에서 개발한 한국어 처리기이다.  \n",
    "얘도 쓰려면 `Okt`모듈을 불러와야 함.  \n",
    "```\n",
    "from konlpy.tag import Okt\n",
    "```\n",
    "\n",
    "### Okt모듈의 함수\n",
    "morphs(pharse)  \n",
    "nouns(pharse)  \n",
    "pos(pharse, stem=Falsem join=False)  \n",
    "nomalize(pharse) : 입력한 문장을 정규화함.  ex) 사랑햌ㅋ -> 사랑해 ㅋㅋ  \n",
    "pharses(pharse) : 입력한 문장에서 어구를 추출함.  ex) 오늘 날씨가 좋아요. -> ['오늘', '오늘 날씨', '날씨']  \n",
    " \n",
    "### Okt 품사 태그 표\n",
    "Noun : 명사  \n",
    "Verb : 동사  \n",
    "Adjective : 형용사  \n",
    "Josa : 조사  \n",
    "Punctuation : 구두점  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'maporphs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f022620fa772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 형태소 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmorphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mokt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaporphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 형태소와 품사 태그 추출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'maporphs' is not defined"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# Okt 형태소 분석기 객체 생성\n",
    "okt = Okt()\n",
    "\n",
    "text = \"아버지가 방에 들어갑니다.\"\n",
    "\n",
    "# 형태소 추출\n",
    "morphs = okt.morphs(text)\n",
    "print(maporphs)\n",
    "\n",
    "# 형태소와 품사 태그 추출\n",
    "pos = okt.pos(text)\n",
    "print(pos)\n",
    "\n",
    "# 명사만 추출\n",
    "nouns = okt.nouns(text)\n",
    "print(nouns)\n",
    "\n",
    "# 정규화, 어구 추출\n",
    "text = \"오늘 날씨가 좋아욬ㅋㅋ\"\n",
    "print(okt.nomalize(text))\n",
    "print(okt.pharses(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('우리', 'NP'), ('챗봇은', 'NA'), ('엔', 'NNB'), ('엘', 'NNP'), ('피', 'NNG'), ('를', 'JKO'), ('좋아하', 'VV'), ('아', 'EC')]\n"
     ]
    }
   ],
   "source": [
    "komoran = Komoran()\n",
    "text = \"우리 챗봇은 엔엘피를 좋아해\"\n",
    "pos = komoran.pos(text)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('우리', 'NP'), ('챗봇은', 'NA'), ('엔', 'NNB'), ('엘', 'NNP'), ('피', 'NNG'), ('를', 'JKO'), ('좋아하', 'VV'), ('아', 'EC')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran(userdic='./user_dic.tsv')\n",
    "text = \"우리 챗봇은 엔엘피를 좋아해\"\n",
    "pos = komoran.pos(text)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4장. 임베딩\n",
    "컴퓨터는 자연어를 직접적으로 처리할 수 없음!  \n",
    "수치연산만 가능해서 자연어를 숫자나 벡터 형태로 변환해야 하는데, 이런 과정을 `임베딩`이라고 함.  \n",
    "즉, 단어나 문장을 수치화해 벡터 공간으로 표현하는 과정을 말함.  \n",
    "임베딩 기법에는 `문장 임베딩`과 `단어 임베딩`이 있음!  \n",
    "\n",
    "\n",
    "## 4.2. 단어 임베딩\n",
    "`단어 임베딩`은 말뭉치에서 각각의 단어를 벡터로 변환하는 기법을 말함.  \n",
    "단어 임베딩은 의미와 문법적 정보를 지니고 있음.  \n",
    "\n",
    "### 4.2.1 원-핫 인코딩\n",
    "`원-핫-인코딩`은 단어를 숫자 벡터로 변환하는 가장 기본적인 방법임.  \n",
    "요소들 중 단 하나의 값만 1이고 나머지 요솟값은 0인 인코딩을 의미함.  \n",
    "원-핫-인코딩으로 나온 결과를 `원-핫-벡터`라고 하고, 전체 요소 중 단 하나만 1이기 때문에 `희소 벡터`라고 함!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '날씨', '구름']\n",
      "{'오늘': 0, '날씨': 1, '구름': 2}\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "import numpy as np\n",
    "\n",
    "komoran = Komoran()\n",
    "text = \"오늘 날씨는 구름이 많아요.\"\n",
    "\n",
    "# 명사만 추출\n",
    "nouns = komoran.nouns(text)\n",
    "print(nouns)\n",
    "\n",
    "# 단어 사전 구축 및 단어별 인덱스 부여\n",
    "dics = {}\n",
    "for word in nouns:\n",
    "    if word not in dics.keys():\n",
    "        dics[word] = len(dics)\n",
    "print(dics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '날씨', '구름']\n",
      "{'오늘': 0, '날씨': 1, '구름': 2}\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "komoran = Komoran()\n",
    "text = \"오늘 날씨는 구름이 많아요.\"\n",
    "\n",
    "# 명사만 추출\n",
    "nouns = komoran.nouns(text)\n",
    "print(nouns)\n",
    "\n",
    "# 단어 사전 구축 및 단어별 인덱스 부여\n",
    "dics = {}\n",
    "for word in nouns:\n",
    "    if word not in dics.keys():\n",
    "        dics[word] = len(dics)\n",
    "print(dics)\n",
    "\n",
    "# 원-핫-인코딩\n",
    "nb_classes = len(dics)\n",
    "targets = list(dics.values())\n",
    "one_hot_targets = np.eye(nb_classes)[targets]\n",
    "print(one_hot_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 희소 표현과 분산 표현\n",
    "단어가 희소벡터로 표현되는 방식을 `희소 표현`이라고 한다.  \n",
    "희소 표현은 각각의 차원이 독립적인 정보를 지니고 있어 사람이 이해하기에 직관적인 장점, 단어 간의 연관성이 전혀 없어 의미를 담을 수 없고, 단어 사전의 크기가 커질 수록 메모리 낭비와 계산 복잡도가 커지는 단점.  \n",
    "\n",
    "자연어 처리를 잘하기 위해서는 기본 토큰이 되는 단어의 의미와 주변 단어 간의 관계가 단어 임베딩에 표현되어야 함.  \n",
    "이를 해결하기 위해 단어 간의 유사성을 잘 표현하면서도 벡터 공간을 절약할 수 있는 방법을 고안했는데, 이를 `분산 표현`이라고 함! \n",
    "\n",
    "### 4.2.3 Word2Vec\n",
    "원-핫-인코딩의 경우에는 구현이 간단하지만, 챗봇의 경우에는 단어간의 유사도를 계산할 수 있어야 좋은 성능을 낸다는 단점이 있다.  \n",
    "그래서 챗봇의 경우에는 원-핫-인코딩은 좋은 기법은 아니다.  \n",
    "그렇기 때문에! 분산 표현 형태의 단어 임베딩 모델을 사용할 것이다.  \n",
    "대표적인 모델로는 `Word2Vec`모델이 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) 더미 데이터 읽기 시작\n",
      "461\n",
      "1) 더미 데이터 읽기 완료 :  0.009504556655883789\n",
      "2) 형태소에서 명사만 추출 시작\n",
      "2) 형태소에서 명사만 추출 완료 :  46.581286668777466\n",
      "3) Word2Vec 모델 학습 시작\n",
      "3) Word2Vec 모델 학습 완료 :  47.0680890083313\n",
      "4) 학습된 모델 저장 시작\n",
      "4) 학습된 모델 저장 완료 :  47.16570067405701\n",
      "corpus_count:  461\n",
      "corpus_total_words :  2463\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from konlpy.tag import Komoran\n",
    "import time\n",
    "\n",
    "# 네이버 영화 리뷰 데이터 읽어옴\n",
    "def read_review_data(filename):\n",
    "    with open('rating_test.txt', 'r') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:]\n",
    "    return data\n",
    "\n",
    "# 학습 시간 측정 시작\n",
    "start = time.time()\n",
    "\n",
    "# 리뷰 파일 읽어오기\n",
    "print('1) 더미 데이터 읽기 시작')\n",
    "review_data = read_review_data('./rating_test.txt')\n",
    "print(len(review_data))\n",
    "print('1) 더미 데이터 읽기 완료 : ', time.time() - start)\n",
    "\n",
    "# 문장 단위로 명사만 추출해 학습 입력 데이터로 만들기\n",
    "print('2) 형태소에서 명사만 추출 시작')\n",
    "komoran = Komoran()\n",
    "docs = [komoran.nouns(sentence[1]) for sentence in review_data]\n",
    "print('2) 형태소에서 명사만 추출 완료 : ', time.time() - start)\n",
    "\n",
    "# Word2Vec 모델 학습\n",
    "print('3) Word2Vec 모델 학습 시작')\n",
    "model = Word2Vec(sentences=docs, vector_size=200, window=4, hs=1, min_count=2, sg=1)\n",
    "print('3) Word2Vec 모델 학습 완료 : ', time.time() - start)\n",
    "\n",
    "# 모델 저장\n",
    "print('4) 학습된 모델 저장 시작')\n",
    "model.save('nvmc.model')\n",
    "print('4) 학습된 모델 저장 완료 : ', time.time() - start)\n",
    "\n",
    "# 학습된 말뭉치 수, 코퍼스 내 전체 단어 수\n",
    "print(\"corpus_count: \", model.corpus_count)\n",
    "print(\"corpus_total_words : \", model.corpus_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_total_words :  2463\n",
      "사랑 :  [ 1.19257662e-02 -1.06058056e-02 -1.00245327e-03  8.25968198e-03\n",
      "  1.48530176e-03 -1.70327872e-02 -7.63928052e-03 -8.83328175e-05\n",
      "  4.72895597e-04  8.73035565e-03 -8.53238627e-04  1.39602215e-03\n",
      " -1.85852672e-03  1.79800428e-02 -1.40954191e-02 -6.63968222e-03\n",
      "  6.77065691e-03 -3.57975205e-03 -9.03997943e-03 -1.55340815e-02\n",
      "  9.60858818e-03 -4.30998346e-03  5.98050514e-03 -1.68791669e-03\n",
      "  2.80035019e-04  8.84239198e-05 -8.49485397e-03 -4.64181975e-03\n",
      " -7.87963346e-03  1.63556263e-02  2.36708932e-02 -1.64566073e-03\n",
      "  9.92840622e-03 -2.17353762e-03 -3.73269897e-03  5.48698846e-03\n",
      "  4.12611943e-03 -9.42074694e-03 -1.29102319e-02 -1.30211813e-02\n",
      " -1.71903726e-02  3.98366479e-03 -5.15509024e-03 -7.17564393e-03\n",
      "  1.81411803e-02 -8.85075703e-03  4.69624763e-04 -1.10276490e-02\n",
      "  1.69543605e-02  1.37519669e-02 -1.23556191e-02 -4.47953306e-03\n",
      " -2.06120498e-03 -1.20374691e-02  3.53826466e-03 -3.00696539e-03\n",
      "  8.79746920e-04 -5.55339968e-03 -1.76490694e-02 -4.09394968e-03\n",
      "  9.10649635e-03 -1.39291887e-03  1.94323249e-03 -3.13297636e-03\n",
      " -2.14838516e-02 -6.92895288e-03 -1.55428592e-02  1.39908753e-02\n",
      " -1.60854310e-02  1.78157613e-02  7.91993737e-03 -9.40424018e-03\n",
      "  2.53536794e-02  5.38917538e-03 -1.13139011e-03 -7.52199034e-04\n",
      " -1.29183754e-03 -5.52555686e-03 -1.70450397e-02  4.44219029e-03\n",
      "  5.04086446e-03 -9.79518611e-03 -2.12184768e-02  2.12840829e-02\n",
      " -1.76762082e-02  6.43314153e-04  2.23776698e-03  8.76317639e-03\n",
      " -8.42244830e-03  6.54612016e-03  1.18670342e-02  1.09924087e-02\n",
      " -3.11994739e-03  8.40503164e-03  1.08369477e-02  1.17852911e-02\n",
      " -7.28940917e-03 -1.15237227e-02  9.87771526e-03  9.01712291e-03\n",
      " -3.24121048e-03  5.29875839e-03  6.30145380e-03 -1.11114625e-02\n",
      " -5.19024348e-03 -1.35117276e-02  3.03106918e-03  9.27165989e-03\n",
      " -9.31053050e-03 -5.92375593e-03 -1.22212106e-02 -2.14989553e-03\n",
      " -4.62687900e-03 -5.97479288e-03  1.23369433e-02 -6.86465716e-03\n",
      "  6.56813197e-03 -1.82821080e-02 -4.92093153e-03 -2.56721601e-02\n",
      " -6.42295321e-03  1.68005638e-02 -1.70883990e-03 -5.00937738e-03\n",
      "  1.92971365e-03  5.15009044e-04 -3.93466745e-03 -1.48432344e-04\n",
      "  5.64739667e-03  2.95794546e-03 -1.62483240e-03  9.08601191e-03\n",
      " -2.65989639e-03  2.32256576e-03 -4.98372037e-03  1.06389392e-02\n",
      " -1.01554049e-02 -1.74215734e-02  2.69115949e-03 -1.06985634e-02\n",
      " -5.70589816e-03 -1.98183712e-02  5.64146508e-03 -8.35716317e-04\n",
      " -1.22644091e-02 -4.17519873e-03 -1.65955853e-02  9.31947678e-03\n",
      " -2.41394434e-03 -6.94186706e-03  6.51485473e-03 -9.08657350e-03\n",
      "  2.86186254e-03  1.04554361e-02 -1.17922984e-02  3.68054211e-03\n",
      "  1.04580559e-02  1.09728249e-02 -5.08140447e-03  5.95970592e-03\n",
      "  1.20398020e-02  2.02369038e-03 -1.05192391e-02  3.66694713e-03\n",
      " -8.30742065e-03  1.08527625e-02  1.12632187e-02 -3.76653927e-03\n",
      " -7.25513790e-03  1.34048183e-02 -7.32966745e-03  7.35850725e-03\n",
      "  3.81013821e-03 -7.23900693e-03 -5.60779031e-03  7.69997388e-03\n",
      " -1.29652862e-02  5.00470027e-03  9.73853841e-03  2.82234419e-03\n",
      " -5.06471097e-03 -2.50726868e-03  4.27257316e-03  7.85452034e-03\n",
      "  1.56599618e-02  9.33611859e-03 -1.61353890e-02  9.42973234e-03\n",
      "  2.02112589e-02 -1.96608528e-03  8.49979185e-03  1.03386566e-02\n",
      "  2.98049417e-03 -1.37615083e-02  4.91994759e-03  1.27553837e-02\n",
      "  1.55112159e-03 -9.82576795e-03 -9.14737768e-03 -3.35279456e-03]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Key '일요일' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-2dae0aac6667>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 단어 유사도 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"일요일 = 월요일\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'일요일'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'월요일'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"안성기 = 배우\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'안성기'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'배우'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"대기업 = 삼성\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'대기업'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'삼성'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m         \"\"\"\n\u001b[0;32m-> 1238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \"\"\"\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \"\"\"\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key '일요일' not present\""
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 모델 로딩\n",
    "model = Word2Vec.load('nvmc.model')\n",
    "print(\"corpus_total_words : \", model.corpus_total_words)\n",
    "\n",
    "# '사랑'이란 단어로 생성한 단어 임베딩 벡터\n",
    "print('사랑 : ', model.wv['사랑'])\n",
    "\n",
    "# 단어 유사도 계산\n",
    "print(\"일요일 = 월요일\\t\", model.wv.similarity(w1='일요일', w2='월요일'))\n",
    "print(\"안성기 = 배우\\t\", model.wv.similarity(w1='안성기', w2='배우'))\n",
    "print(\"대기업 = 삼성\\t\", model.wv.similarity(w1='대기업', w2='삼성'))\n",
    "print(\"일요일 != 삼성\\t\", model.wv.similarity(w1='일요일', w2='삼성'))\n",
    "print(\"히어로 != 삼성\\t\", model.wv.similarity(w1='히어로', w2='삼성'))\n",
    "\n",
    "\n",
    "\n",
    "# 가장 유사한 단어 추출\n",
    "print(model.wv.most_similar(\"안성기\", topn=5))\n",
    "print(model.wv.most_similar(\"시리즈\", topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5장. 텍스트 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
